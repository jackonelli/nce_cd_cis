{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "006eb13f",
   "metadata": {},
   "source": [
    "# Choice of proposal distribution\n",
    "\n",
    "We investigate the effect of the proposal distribution when learning an unnormalised model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa9943c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from pathlib import Path\n",
    "from datasets import (\n",
    "    ToyDataset,\n",
    ")  # (this needs to be imported before torch, because cv2 needs to be imported before torch for some reason)\n",
    "from ebmdn_model_K4 import ToyNet\n",
    "\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch.distributions\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "import matplotlib\n",
    "\n",
    "# matplotlib.use(\"TkAgg\")\n",
    "# matplotlib.use(\"Agg\")\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# NOTE! change this to not overwrite all log data when you train the model:\n",
    "RESULTS_DIR = Path(\"1dregression_1/results/mdn_k4/\")\n",
    "EXPERIMENT_NAME = \"mdn_k4\"\n",
    "CD = \"ebm_CD\"\n",
    "MDN = \"ebm_MDN\"\n",
    "\n",
    "NUM_EPOCHS = 25\n",
    "BATCH_SIZE = 32\n",
    "LR = 0.001\n",
    "\n",
    "NUM_SAMPLES = 32\n",
    "NUM_MODELS = 15\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "626e8a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "from functools import partial\n",
    "import math\n",
    "import pickle\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch.distributions\n",
    "\n",
    "from scipy.linalg import sqrtm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "sys.path.append(\"../../ebms_proposals/1dregression_1\")\n",
    "from datasets import (\n",
    "    ToyDataset,\n",
    ")  # (this needs to be imported before torch, because cv2 needs to be imported before torch for some reason)\n",
    "from ebmdn_model_K4 import ToyNet\n",
    "from src.nce.cnce import CondNceCrit\n",
    "from src.nce.rank import NceRankCrit\n",
    "\n",
    "from src.noise_distr.normal import MultivariateNormal\n",
    "from src.models.gaussian_model import DiagGaussianModel\n",
    "\n",
    "from src.training.model_training import train_model, train_model_model_proposal\n",
    "from src.data.normal import MultivariateNormalData\n",
    "from src.training.training_utils import Mse, MvnKlDiv, no_change_stopping_condition, no_stopping\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# NOTE! change this to not overwrite all log data when you train the model:\n",
    "RESULTS_DIR = Path(\"./results/\")\n",
    "EXPERIMENT_NAME = \"mdn_k4\"\n",
    "CD = \"ebm_CD\"\n",
    "MDN = \"ebm_MDN\"\n",
    "\n",
    "NUM_EPOCHS = 12\n",
    "BATCH_SIZE = 32\n",
    "LR = 0.001\n",
    "\n",
    "NUM_SAMPLES = 32\n",
    "NUM_MODELS = 15\n",
    "\n",
    "def mvn_curve(mu, cov, std=1, res=100):\n",
    "    with torch.no_grad():\n",
    "        angles = torch.linspace(0, 2*torch.pi, res)\n",
    "        curve_param = torch.column_stack((torch.cos(angles), torch.sin(angles)))\n",
    "        ellipsis = std * curve_param @ torch.Tensor(sqrtm(cov))\n",
    "        return mu + ellipsis\n",
    "    \n",
    "def plot_mvn(levels, ax, label):\n",
    "    ax.plot(levels[:, 0], levels[:, 1], label=label)\n",
    "\n",
    "def plot_distrs_ideal(p_d, p_t_d, p_t_t):    \n",
    "    fig, ax = plt.subplots()\n",
    "    ax.set_xlim([-3, 10])\n",
    "    ax.set_ylim([-3, 10])\n",
    "    distrs = [\n",
    "        (p_d.mu, p_d.cov, \"$p_{d}}$\"),\n",
    "        (p_t_d.mu, p_t_d.cov(), \"$q=p_d$\"),\n",
    "        (p_t_t.mu, p_t_t.cov(), \"$q = p_{\\\\theta}$\")\n",
    "    ]\n",
    "    for mu, cov, label in distrs:\n",
    "        plot_mvn(mvn_curve(mu, cov), ax, label)\n",
    "    ax.set_title(\"Comparison, optimal proposal distrs.\")\n",
    "    ax.legend()\n",
    "\n",
    "def plot_distrs_adaptive(p_d, p_theta, q_phi):    \n",
    "    fig, ax = plt.subplots()\n",
    "    ax.set_xlim([-3, 10])\n",
    "    ax.set_ylim([-3, 10])\n",
    "    distrs = [\n",
    "        (p_d.mu, p_d.cov, \"$p_{d}}$\"),\n",
    "        (p_theta.mu, p_theta.cov(), \"$p_{\\\\theta}$\"),\n",
    "        (q_phi.mu, q_phi.cov(), \"$q_{\\\\varphi}$\")\n",
    "    ]\n",
    "    for mu, cov, label in distrs:\n",
    "        plot_mvn(mvn_curve(mu, cov), ax, label)\n",
    "    ax.set_title(\"Adaptive proposal\")\n",
    "    ax.legend()\n",
    "    \n",
    "repo_dir = Path.cwd().parent\n",
    "exp_dir = repo_dir / \"1d_toy\"\n",
    "exp_dir.exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd178b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "\n",
    "import scipy.stats\n",
    "\n",
    "# import pickle\n",
    "\n",
    "################################################################################\n",
    "# run this once to generate the training data:\n",
    "################################################################################\n",
    "\n",
    "def generate_data(data_dir: Optional[Path]):\n",
    "    x = np.random.uniform(low=-3.0, high=3.0, size=(2000,))\n",
    "    x = x.astype(np.float32)\n",
    "\n",
    "    y = []\n",
    "    for x_value in x:\n",
    "        if x_value < 0:\n",
    "            component = np.random.randint(low=1, high=6)  # (1, 2, 3, 4, 5 with 0.5 prob)\n",
    "\n",
    "            if component in [1, 2, 3, 4]:\n",
    "                mu_value = np.sin(x_value)\n",
    "                sigma_value = 0.15 * (1.0 / (1 + 1))\n",
    "            elif component == 5:\n",
    "                mu_value = -np.sin(x_value)\n",
    "                sigma_value = 0.15 * (1.0 / (1 + 1))\n",
    "\n",
    "            y_value = np.random.normal(mu_value, sigma_value)\n",
    "        else:\n",
    "            y_value = np.random.lognormal(0.0, 0.25) - 1.0\n",
    "\n",
    "        y.append(y_value)\n",
    "    y = np.array(y, dtype=np.float32)\n",
    "    if data_dir is not None:\n",
    "        with open(data_dir / \"x.pkl\", \"wb\") as file:\n",
    "            pickle.dump(x, file)\n",
    "        with open(data_dir / \"y.pkl\", \"wb\") as file:\n",
    "            pickle.dump(y, file)\n",
    "    return x, y\n",
    "        \n",
    "def generate_scores(data_dir: Optional[Path]):\n",
    "    num_samples = 2048\n",
    "    x = np.linspace(-3.0, 3.0, num_samples, dtype=np.float32)\n",
    "    y_samples = np.linspace(-3.0, 3.0, num_samples)  # (shape: (num_samples, ))\n",
    "    x_values_2_scores = {}\n",
    "    for x_value in x:\n",
    "        if x_value < 0:\n",
    "            scores = 0.8 * scipy.stats.norm.pdf(\n",
    "                y_samples, np.sin(x_value), 0.15 * (1.0 / (1 + 1))\n",
    "            ) + 0.2 * scipy.stats.norm.pdf(\n",
    "                y_samples, -np.sin(x_value), 0.15 * (1.0 / (1 + 1))\n",
    "            )\n",
    "        else:\n",
    "            scores = scipy.stats.lognorm.pdf(y_samples + 1.0, 0.25)\n",
    "\n",
    "        x_values_2_scores[x_value] = scores\n",
    "    if data_dir is not None:\n",
    "        with open(data_dir / \"gt_x_values_2_scores.pkl\", \"wb\") as file:\n",
    "            pickle.dump(x_values_2_scores, file)\n",
    "    return x, x_values_2_scores\n",
    "\n",
    "\n",
    "def save_checkpoint(network, model_id, model_idx, epoch):\n",
    "    # save the model weights to disk:\n",
    "    checkpoint_path = (\n",
    "        network.checkpoints_dir / f\"model_{model_id}_{model_idx}_epoch_{epoch+1}.pth\"\n",
    "    )\n",
    "    torch.save(network.state_dict(), checkpoint_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d68134a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data.generic import Generic\n",
    "x, y = generate_data(None)\n",
    "x, y = torch.tensor(x), torch.tensor(y)\n",
    "x_range, scores = generate_scores(None)\n",
    "plt.plot(x, y, 'k.')\n",
    "# plt.plot(x_range, scores, 'k.')\n",
    "\n",
    "training_data = Generic(torch.column_stack((x,y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f7f327",
   "metadata": {},
   "source": [
    "# Common setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0889754c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import LinearLR\n",
    "NUM_EPOCHS = 50\n",
    "\n",
    "def get_train_loader(batch_size):\n",
    "    train_dataset = ToyDataset()\n",
    "\n",
    "    num_train_batches = int(len(train_dataset) / batch_size)\n",
    "    print(\"num_train_batches:\", num_train_batches)\n",
    "\n",
    "    return torch.utils.data.DataLoader(\n",
    "        dataset=train_dataset, batch_size=batch_size, shuffle=True\n",
    "    )\n",
    "\n",
    "def save_checkpoint(network, model_id, model_idx, epoch):\n",
    "    # save the model weights to disk:\n",
    "    checkpoint_path = (\n",
    "        network.checkpoints_dir / f\"model_{model_id}_{model_idx}_epoch_{epoch+1}.pth\"\n",
    "    )\n",
    "    torch.save(network.state_dict(), checkpoint_path)\n",
    "\n",
    "def train_model_cd_obj(train_loader, model_id, model_idx):\n",
    "    network = ToyNet(f\"{model_id}_{model_idx}\", project_dir=RESULTS_DIR).cuda()\n",
    "    p_optimizer = torch.optim.SGD(network.parameters(), lr=0.1)\n",
    "    q_optimizer = torch.optim.SGD(network.parameters(), lr=0.1)\n",
    "\n",
    "    epoch_losses_train = torch.empty((NUM_EPOCHS,))\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "\n",
    "        network.train()  # (set in training mode, this affects BatchNorm and dropout)\n",
    "        batch_losses = []\n",
    "        for _, (xs, ys) in enumerate(train_loader):\n",
    "            xs = xs.cuda().unsqueeze(1)  # (shape: (batch_size, 1))\n",
    "            ys = ys.cuda().unsqueeze(1)  # (shape: (batch_size, 1))\n",
    "\n",
    "            x_features = network.feature_net(xs)  # (shape: (batch_size, hidden_dim))\n",
    "            means, log_sigma2s, weights = network.noise_net(\n",
    "                x_features.detach()\n",
    "            )  # (all have shape: (batch_size, K))\n",
    "            sigmas = torch.exp(log_sigma2s / 2.0)  # (shape: (batch_size, K))\n",
    "            # print(\"Sigmas\", sigmas)\n",
    "            q_distr = torch.distributions.normal.Normal(loc=means, scale=sigmas)\n",
    "            q_ys_K = torch.exp(\n",
    "                q_distr.log_prob(torch.transpose(ys, 1, 0).unsqueeze(2))\n",
    "            )  # (shape: (1, batch_size, K))\n",
    "            q_ys = torch.sum(\n",
    "                weights.unsqueeze(0) * q_ys_K, dim=2\n",
    "            )  # (shape: (1, batch_size))\n",
    "            q_ys = q_ys.squeeze(0)  # (shape: (batch_size))\n",
    "\n",
    "            y_samples_K = q_distr.sample(\n",
    "                sample_shape=torch.Size([NUM_SAMPLES])\n",
    "            )  # (shape: (num_samples, batch_size, K))\n",
    "            inds = torch.multinomial(\n",
    "                weights, num_samples=NUM_SAMPLES, replacement=True\n",
    "            ).unsqueeze(\n",
    "                2\n",
    "            )  # (shape: (batch_size, num_samples, 1))\n",
    "            inds = torch.transpose(inds, 1, 0)  # (shape: (num_samples, batch_size, 1))\n",
    "            y_samples = y_samples_K.gather(2, inds).squeeze(\n",
    "                2\n",
    "            )  # (shape: (num_samples, batch_size))\n",
    "            y_samples = y_samples.detach()\n",
    "            q_y_samples_K = torch.exp(\n",
    "                q_distr.log_prob(y_samples.unsqueeze(2))\n",
    "            )  # (shape: (num_samples, batch_size, K))\n",
    "            q_y_samples = torch.sum(\n",
    "                weights.unsqueeze(0) * q_y_samples_K, dim=2\n",
    "            )  # (shape: (num_samples, batch_size))\n",
    "            y_samples = torch.transpose(\n",
    "                y_samples, 1, 0\n",
    "            )  # (shape: (batch_size, num_samples))\n",
    "            q_y_samples = torch.transpose(\n",
    "                q_y_samples, 1, 0\n",
    "            )  # (shape: (batch_size, num_samples))\n",
    "\n",
    "            scores_gt = network.predictor_net(\n",
    "                x_features, ys\n",
    "            )  # (shape: (batch_size, 1))\n",
    "            scores_gt = scores_gt.squeeze(1)  # (shape: (batch_size))\n",
    "\n",
    "            scores_samples = network.predictor_net(\n",
    "                x_features, y_samples\n",
    "            )  # (shape: (batch_size, num_samples))\n",
    "\n",
    "            # EBM loss\n",
    "            f_samples = scores_samples\n",
    "            p_N_samples = q_y_samples.detach()\n",
    "            f_0 = scores_gt\n",
    "            p_N_0 = q_ys.detach()\n",
    "            exp_vals_0 = f_0 - torch.log(p_N_0 + 0.0)\n",
    "            exp_vals_samples = f_samples - torch.log(p_N_samples + 0.0)\n",
    "            exp_vals = torch.cat([exp_vals_0.unsqueeze(1), exp_vals_samples], dim=1)\n",
    "            loss_ebm_nce = -torch.mean(exp_vals_0 - torch.logsumexp(exp_vals, dim=1))\n",
    "\n",
    "            # Prop loss\n",
    "            # Compute weights with detached tensors\n",
    "            p_tilde_0 = torch.exp(scores_gt.detach())\n",
    "            p_tilde_1_J = torch.exp(scores_samples.detach())\n",
    "            ps = torch.column_stack((p_tilde_0, p_tilde_1_J))\n",
    "            qs_no_grad = torch.column_stack((p_N_0, p_N_samples))\n",
    "            w_tilde = ps / qs_no_grad\n",
    "            w_norm = w_tilde / w_tilde.sum(axis=1).unsqueeze(-1)\n",
    "            # Assemble log q(x_0:J)\n",
    "            log_qs = torch.log(torch.column_stack((q_ys, q_y_samples)))\n",
    "\n",
    "            # print(f\"p_0: {p_tilde_0.shape}, p_1:J: {p_tilde_1_J.shape}\")\n",
    "            # print(f\"q_0: {p_N_0.shape}, q_1:J: {p_N_samples.shape}\")\n",
    "            # print(f\"q_ys: {q_ys.shape}, q_y_samples: {q_y_samples.shape}\")\n",
    "            # print(f\"Z_cis: {Z_cis.shape}\")\n",
    "            loss_mdn_kl = -(w_norm * log_qs).sum(axis=1).mean()\n",
    "\n",
    "            # log_Z = torch.logsumexp(\n",
    "            #     scores_samples.detach() - torch.log(q_y_samples), dim=1\n",
    "            # ) - math.log(\n",
    "            #     NUM_SAMPLES\n",
    "            # )  # (shape: (batch_size))\n",
    "            # loss_mdn_kl = torch.mean(log_Z)\n",
    "\n",
    "            loss_mdn_nll = torch.mean(-torch.log(q_ys))\n",
    "\n",
    "            loss = loss_ebm_nce + loss_mdn_kl\n",
    "\n",
    "            # loss_value = loss.data.cpu().numpy()\n",
    "            batch_losses.append(loss.data.cpu().item())\n",
    "\n",
    "            ########################################################################\n",
    "            # optimization step:\n",
    "            ########################################################################\n",
    "            optimizer.zero_grad()  # (reset gradients)\n",
    "            loss.backward()  # (compute gradients)\n",
    "            optimizer.step()  # (perform optimization step)\n",
    "        epoch_loss = torch.tensor(batch_losses).mean().item()\n",
    "        epoch_losses_train[epoch] = epoch_loss\n",
    "        print(f\"Train loss: {epoch_loss}\")\n",
    "        save_checkpoint(network, model_id, model_idx, epoch)\n",
    "\n",
    "    # save_loss(\"cd\", network.model_dir, epoch_losses_train)\n",
    "    return epoch_losses_train\n",
    "\n",
    "\n",
    "    # save_loss(\"mdn\", network.model_dir, epoch_losses_train)\n",
    "    return epoch_losses_train\n",
    "idx = 0\n",
    "epoch_losses_mdn = train_model_cd_obj(get_train_loader(BATCH_SIZE), CD, idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b549c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from shutil import copyfile\n",
    "src_dir = Path.cwd().parent.parent / \"ebms_proposals/1dregression_1/results/mdn_k4/training_logs/\"\n",
    "dst_dir = Path.cwd() / \"losses\"\n",
    "\n",
    "for i in range(15):\n",
    "    src = src_dir / f\"model_ebm_CD_{i}/epoch_losses_train.pkl\"\n",
    "    copyfile(src, dst_dir / f\"cd_{i}.pkl\")\n",
    "    src = src_dir / f\"model_ebm_MDN_{i}/epoch_losses_train.pkl\"\n",
    "    copyfile(src, dst_dir / f\"mdn_{i}.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c215b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from pathlib import Path\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "matplotlib.use(\"TkAgg\")\n",
    "\n",
    "dir_ = Path.cwd().parent.parent / \"ebms_proposals/1dregression_1/results\"\n",
    "\n",
    "runs, num_epochs = 5, 30\n",
    "cis_p_loss = np.load(dir_ / f\"{runs}_runs_{num_epochs}_epochs/cis_p_loss.npy\")\n",
    "cis_q_loss = np.load(dir_ / f\"{runs}_runs_{num_epochs}_epochs/cis_q_loss.npy\")\n",
    "cis_q_nll = np.load(dir_ / f\"{runs}_runs_{num_epochs}_epochs/cis_q_nll.npy\")\n",
    "cis_p_nll = np.load(dir_ / f\"{runs}_runs_{num_epochs}_epochs/cis_p_nll.npy\")\n",
    "\n",
    "is_p_loss = np.load(dir_ / f\"{runs}_runs_{num_epochs}_epochs/is_p_loss.npy\")\n",
    "is_q_loss = np.load(dir_ / f\"{runs}_runs_{num_epochs}_epochs/is_q_loss.npy\")\n",
    "is_q_nll = np.load(dir_ / f\"{runs}_runs_{num_epochs}_epochs/is_q_nll.npy\")\n",
    "is_p_nll = np.load(dir_ / f\"{runs}_runs_{num_epochs}_epochs/is_p_nll.npy\")\n",
    "\n",
    "#is_ = np.load(dir_ / f\"{num_epochs}_epochs/is_losses.npy\")\n",
    "\n",
    "epochs = np.arange(1, num_epochs+1)\n",
    "is_p_loss_mean, is_p_loss_std = np.mean(is_p_loss, axis=0), np.std(is_p_loss, axis=0)\n",
    "is_q_loss_mean, is_q_loss_std = np.mean(is_q_loss, axis=0), np.std(is_q_loss, axis=0)\n",
    "is_q_nll_mean, is_q_nll_std = np.mean(is_q_nll, axis=0), np.std(is_q_nll, axis=0)\n",
    "is_p_nll_mean, is_p_nll_std = np.mean(is_p_nll, axis=0), np.std(is_p_nll, axis=0)\n",
    "\n",
    "\n",
    "cis_p_loss_mean, cis_p_loss_std = np.mean(cis_p_loss, axis=0), np.std(cis_p_loss, axis=0)\n",
    "cis_q_loss_mean, cis_q_loss_std = np.mean(cis_q_loss, axis=0), np.std(cis_q_loss, axis=0)\n",
    "cis_q_nll_mean, cis_q_nll_std = np.mean(cis_q_nll, axis=0), np.std(cis_q_nll, axis=0)\n",
    "cis_p_nll_mean, cis_p_nll_std = np.mean(cis_p_nll, axis=0), np.std(cis_p_nll, axis=0)\n",
    "\n",
    "_, (ax_loss, ax_nll) = plt.subplots(1, 2)\n",
    "ax_loss.errorbar(epochs, is_p_loss_mean, is_p_loss_std / np.sqrt(num_epochs), label=\"Loss $p_{IS}$\")\n",
    "ax_loss.errorbar(epochs, is_q_loss_mean, is_q_loss_std / np.sqrt(num_epochs), label=\"Loss $q_{IS}$\")\n",
    "ax_loss.errorbar(epochs, cis_p_loss_mean, cis_p_loss_std / np.sqrt(num_epochs), label=\"Loss $p_{CIS}$\")\n",
    "ax_loss.errorbar(epochs, cis_q_loss_mean, cis_q_loss_std / np.sqrt(num_epochs), label=\"Loss $q_{CIS}$\")\n",
    "ax_loss.set_ylabel(\"Loss\")\n",
    "ax_loss.set_xlabel(\"Epoch\")\n",
    "ax_loss.legend()\n",
    "\n",
    "ax_nll.errorbar(epochs, is_q_nll_mean, is_q_nll_std / np.sqrt(num_epochs), label=\"NLL $q_{IS}$\")\n",
    "ax_nll.errorbar(epochs, cis_q_nll_mean, cis_q_nll_std / np.sqrt(num_epochs), label=\"NLL $q_{CIS}$\")\n",
    "#ax_nll.errorbar(epochs, is_p_nll_mean, is_p_nll_std / np.sqrt(num_epochs), label=\"NLL $p_{IS}$\")\n",
    "#ax_nll.errorbar(epochs, cis_p_nll_mean, cis_p_nll_std / np.sqrt(num_epochs), label=\"NLL $p_{CIS}$\")\n",
    "ax_nll.set_ylabel(\"NLL\")\n",
    "ax_nll.set_xlabel(\"Epoch\")\n",
    "ax_nll.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b129047",
   "metadata": {},
   "outputs": [],
   "source": [
    "is_q_nll"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
