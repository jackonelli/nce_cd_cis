{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b21d34e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "from src.nce.cond_cd_cnce import CondCdCnceCrit\n",
    "from src.nce.cond_per_cnce import CondPersistentCnceCrit\n",
    "\n",
    "from src.noise_distr.conditional_bernoulli import ConditionalMultivariateBernoulli\n",
    "\n",
    "from src.models.ebm.two_layer_ebm import Ebm\n",
    "from src.models.ebm.cond_two_layer_ebm import CondEbm\n",
    "from src.data.mnist import MnistDataset\n",
    "from src.data.mnist_w_labels import MnistDatasetWLabs\n",
    "\n",
    "from src.training.model_training import train_model\n",
    "from src.training.training_utils import no_stopping\n",
    "\n",
    "from src.experiments.mnist_exp_utils import initialise_ebm_params\n",
    "from src.experiments.mnist_exp_utils import initialise_cond_ebm_params\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4307bfe",
   "metadata": {},
   "source": [
    "## EXPERIMENTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa4eb4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data specs\n",
    "img_dim = 28\n",
    "num_dims = img_dim**2\n",
    "num_classes = 1\n",
    "\n",
    "# Model specs\n",
    "num_hidden = 500\n",
    "num_hidden_2 = 100\n",
    "\n",
    "# Training specs\n",
    "num_neg_samples = 5\n",
    "lr = 0.1\n",
    "batch_size = 64\n",
    "num_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40698ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check accuracy of model\n",
    "def rbm_acc(rbm, data_loader, k=100):\n",
    "    acc = 0\n",
    "    for i, (y, idx) in enumerate(data_loader, 0):\n",
    "        _, y_pred = rbm.sample(y, k=k)\n",
    "        acc += (y_pred == y).type(torch.float).mean(dim=-1).sum()\n",
    "        \n",
    "    return acc / len(data_loader.dataset)\n",
    "\n",
    "def placeholder_metric(model):\n",
    "    return model.input_weights.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "655c310d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurations to consider in experiments \n",
    "\n",
    "config_conditional_multi = {\n",
    "    \"criterion\": CondCdCnceCrit,\n",
    "    \"label\": \"cd_cnce\",\n",
    "    \"estimate_part_fn\": False,\n",
    "    \"conditional_noise_distr\": True,\n",
    "    \"mcmc_steps\": 1,\n",
    "}\n",
    "\n",
    "config_per_cnce = {\n",
    "    \"criterion\": CondPersistentCnceCrit,\n",
    "    \"label\": \"pers_cd_cnce\",\n",
    "    \"estimate_part_fn\": False,\n",
    "    \"conditional_noise_distr\": True,\n",
    "    \"mcmc_steps\": None,\n",
    "}\n",
    "\n",
    "configs = [config_conditional_multi]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82c8c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get data \n",
    "training_data = MnistDatasetWLabs(train=True, root_dir=\"../src/data/datasets/\", num_classes=num_classes)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(training_data, batch_size=batch_size, shuffle=True)\n",
    "                             \n",
    "test_data = MnistDatasetWLabs(train=False, root_dir=\"../src/data/datasets/\", num_classes=num_classes)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "090b9998",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise model       \n",
    "input_weights, input_bias, hidden_weights, hidden_bias, input_class_weights, input_class_bias, \\\n",
    "           hidden_class_weights, hidden_class_bias = initialise_cond_ebm_params(num_visible=num_dims, \n",
    "                                                                                    num_hidden=num_hidden,\n",
    "                                                                                    num_conditional=num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "935b71c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cond_bern_params(y, eps=1e-1):\n",
    "    num_dims = y.shape[-1]\n",
    "    p_0, p_1 = torch.zeros((num_dims,)), torch.zeros((num_dims,))\n",
    "    for i in range(num_dims):\n",
    "                \n",
    "        if y[y[:, i] < 0.5, i].size()[0] == 0:\n",
    "            p_0[i] = 0.5\n",
    "        else:\n",
    "            p_0[i] = y[y[:, i] < 0.5, i].mean()\n",
    "        \n",
    "        if y[y[:, i] >= 0.5, i].size()[0] == 0:\n",
    "            p_1[i] = 0.5\n",
    "        else:\n",
    "            p_1[i] = y[y[:, i] >= 0.5, i].mean()\n",
    "\n",
    "    p_0[p_0 < eps] = eps\n",
    "    p_1[p_1 > 1 - eps] = 1 - eps\n",
    "    return p_0, p_1\n",
    "        \n",
    "p_0, p_1 = get_cond_bern_params(training_data.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c568dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test noise distr. params\n",
    "\n",
    "fig, ax = plt.subplots(2, 1)\n",
    "\n",
    "num_samples = 16\n",
    "y_true = training_data.y[:num_samples, :].clone()\n",
    "y_true[y_true >= 0.5] = 1.0\n",
    "y_true[y_true < 0.5] = 0.0\n",
    "y_sample = (1 - y_true) * torch.distributions.Bernoulli(p_0).sample((1,)) + y_true * torch.distributions.Bernoulli(p_1).sample((1,))\n",
    "\n",
    "\n",
    "ax[0].imshow(np.transpose(torchvision.utils.make_grid(y_true.reshape(-1, 1, 28, 28), nrow=4).numpy(), (1, 2, 0)))\n",
    "ax[1].imshow(np.transpose(torchvision.utils.make_grid(y_sample.reshape(-1, 1, 28, 28), nrow=4).numpy(), (1, 2, 0)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a08bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run experiments\n",
    "\n",
    "for config in configs:\n",
    "    \n",
    "    # Make sure that these are \"reinitialised\"\n",
    "    p_m, p_n, criterion = None, None, None\n",
    "\n",
    "    #p_m = Ebm(input_weights=input_weights.clone(), input_bias=input_bias.clone(), hidden_weights=hidden_weights.clone(),\n",
    "    #          hidden_bias=hidden_bias.clone())\n",
    "    p_m = CondEbm(input_weights=input_weights.clone(), input_bias=input_bias.clone(), hidden_weights=hidden_weights.clone(),\n",
    "                  hidden_bias=hidden_bias.clone(), input_class_weights=input_class_weights.clone(),\n",
    "                  input_class_bias=input_class_bias.clone(), hidden_class_weights=hidden_class_weights.clone(), \n",
    "                  hidden_class_bias=hidden_class_bias.clone())\n",
    "    \n",
    "    if config[\"conditional_noise_distr\"]:\n",
    "        p_n = ConditionalMultivariateBernoulli(p_0, p_1)\n",
    "    else:\n",
    "        p_n = None\n",
    "   \n",
    "    if config[\"mcmc_steps\"] is not None:\n",
    "        criterion = config[\"criterion\"](p_m, p_n, num_neg_samples, config[\"mcmc_steps\"])\n",
    "    else:\n",
    "        criterion = config[\"criterion\"](p_m, p_n, num_neg_samples)\n",
    "\n",
    "    save_dir = None\n",
    "    _ = train_model(criterion, placeholder_metric, train_loader, save_dir, num_epochs=num_epochs,\n",
    "                    decaying_lr=True, weight_decay=1e-3, stopping_condition=no_stopping)\n",
    "    \n",
    "    # Save model\n",
    "    torch.save(p_m.state_dict(), \"res/params_ebm_\" + config[\"label\"])\n",
    "    \n",
    "    # Check test accuracy of model\n",
    "    #acc = rbm_acc(p_m, test_loader)\n",
    "    #print(\"Model accuracy: {}\".format(acc))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489b6037",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise results\n",
    "\n",
    "num_samples = 4\n",
    "fig, ax = plt.subplots(len(configs), 2)\n",
    "\n",
    "\n",
    "if ax.ndim == 1:\n",
    "    ax = ax.reshape(1, -1)\n",
    "    \n",
    "p_m = CondEbm(input_weights=input_weights.clone(), input_bias=input_bias.clone(), hidden_weights=hidden_weights.clone(),\n",
    "              hidden_bias=hidden_bias.clone(), input_class_weights=input_class_weights.clone(),\n",
    "              input_class_bias=input_class_bias.clone(), hidden_class_weights=hidden_class_weights.clone(), \n",
    "              hidden_class_bias=hidden_class_bias.clone())\n",
    "\n",
    "for i, config in enumerate(configs):\n",
    "    p_m.load_state_dict(torch.load(\"res/params_ebm_\" + config[\"label\"]))\n",
    "    \n",
    "    \n",
    "    y_pred = p_m.sample(num_samples=num_samples, num_epochs=1000)\n",
    "\n",
    "    ax[i, 1].imshow(np.transpose(torchvision.utils.make_grid(y_pred.reshape(-1, 1, 28, 28), nrow=4).numpy(), (1, 2, 0))) \n",
    "    ax[i, 1].set_title(config[\"label\"] + \" generated\")\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb931ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Sampla bilder istället (i dataloader); makes more sense om jag vill sampla här i slutet? Men då bör jag göra det för brusparams också?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
