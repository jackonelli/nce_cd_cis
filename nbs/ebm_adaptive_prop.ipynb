{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "006eb13f",
   "metadata": {},
   "source": [
    "# Choice of proposal distribution\n",
    "\n",
    "We investigate the effect of the proposal distribution when learning an unnormalised model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "626e8a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from functools import partial\n",
    "import torch\n",
    "from scipy.linalg import sqrtm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "from src.nce.rank import NceRankCrit\n",
    "\n",
    "from src.data.normal import MultivariateNormal\n",
    "from src.models.ebm import Ebm\n",
    "from src.noise_distr.adaptive import AdaptiveMdn\n",
    "from src.nce.adaptive_rank import AdaptiveRankKernel\n",
    "\n",
    "from src.training.model_training import train_model, train_model_model_proposal\n",
    "from src.data.normal import MultivariateNormalData\n",
    "from src.training.training_utils import Mse, MvnKlDiv, no_change_stopping_condition, no_stopping\n",
    "from src.experiments.utils import mvn_curve, plot_mvn, plot_distrs_adaptive\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "408ab813",
   "metadata": {},
   "source": [
    "# Common setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfaef899",
   "metadata": {},
   "outputs": [],
   "source": [
    "D, N, J = 2, 100, 10 # Dimension, Num. data samples, Num neg. samples\n",
    "mu_star, cov_star = torch.ones(D,), torch.eye(D)\n",
    "\n",
    "# Data distribution\n",
    "p_d = MultivariateNormal(mu_star, cov_star)\n",
    "# Model distribution\n",
    "# init_mu, init_cov =5.0*torch.ones(D,), 4.0*torch.eye(D)\n",
    "\n",
    "# Optimisation\n",
    "num_epochs = 100\n",
    "batch_size = 16\n",
    "learn_rate = 0.05*batch_size**0.5\n",
    "scheduler_opts = (30, 0.9)\n",
    "\n",
    "# Metrics\n",
    "kl_div = MvnKlDiv(p_d.mu, p_d.cov).metric\n",
    "mse = Mse(p_d.mu).metric\n",
    "metric = kl_div"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae63a3f",
   "metadata": {},
   "source": [
    "# Adaptive proposal\n",
    "\n",
    "Assume that we have a learnable proposal $q_\\varphi$.\n",
    "We jointly learn this proposal by minimising\n",
    "$$\n",
    "KL(p_\\theta \\| q_\\varphi) \\propto - \\mathbb{E}_{x \\sim p_\\theta} \\log q_\\varphi(x) = \\mathcal{L}_\\varphi\n",
    "$$\n",
    "with,\n",
    "$$\n",
    "\\nabla_\\varphi \\mathcal{L}_\\varphi \\approx - \\sum_{j=0}^J w(x_j) \\nabla \\log q_\\varphi(x_j)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd8b3b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "q_phi = AdaptiveMdn(num_components=4, input_dim=2)\n",
    "\n",
    "x = torch.randn(batch_size, D)\n",
    "means, sigmas, weights = q_phi.predict_params(x)\n",
    "inds = q_phi.sample(torch.Size((x.size(0), J)), x)\n",
    "for ind in inds:\n",
    "    print(ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e44b2b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_adaptive_proposal(\n",
    "    p_theta,\n",
    "    q_phi,\n",
    "    p_criterion,\n",
    "    q_criterion,\n",
    "    evaluation_metric,\n",
    "    train_loader,\n",
    "    save_dir,\n",
    "    neg_sample_size,\n",
    "    num_epochs,\n",
    "    stopping_condition=no_stopping,\n",
    "    lr: float = 0.1,\n",
    "):\n",
    "    \"\"\"Training loop for adaptive proposal q_phi\n",
    "\n",
    "    Training loop for jointly learning p_tilde_theta and q_phi.\n",
    "    Where we assume that we can sample and evaluate q_phi.\n",
    "    \"\"\"\n",
    "    p_optimizer = torch.optim.SGD(p_theta.parameters(), lr=lr)\n",
    "    q_optimizer = torch.optim.SGD(q_phi.parameters(), lr=lr)\n",
    "    batch_metrics = []\n",
    "    batch_metrics.append(evaluation_metric(p_theta))\n",
    "    batch_losses = []\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        # print(f\"Epoch {epoch}\")\n",
    "        old_params = torch.nn.utils.parameters_to_vector(q_phi.parameters())\n",
    "        for _, (y, idx) in enumerate(train_loader, 0):\n",
    "\n",
    "            #with torch.no_grad():\n",
    "            #    p_loss = p_criterion.crit(y, None)\n",
    "            #    batch_losses.append(p_loss.item())\n",
    "            # Calculate and assign gradients\n",
    "            p_optimizer.zero_grad()\n",
    "            p_criterion.calculate_crit_grad(y, idx)\n",
    "            p_optimizer.step()\n",
    "\n",
    "            q_optimizer.zero_grad()\n",
    "            q_criterion.calculate_crit_grad(y, idx)\n",
    "            q_optimizer.step()\n",
    "            with torch.no_grad():\n",
    "                batch_metrics.append(evaluation_metric(p_theta))\n",
    "            \n",
    "        if stopping_condition(\n",
    "            torch.nn.utils.parameters_to_vector(q_phi.parameters()), old_params\n",
    "        ):\n",
    "            print(\"Training converged\")\n",
    "            break\n",
    "    return torch.tensor(batch_losses), torch.tensor(batch_metrics)\n",
    "\n",
    "input_dim = mu_star.size(0)\n",
    "p_theta = Ebm(input_dim = input_dim)\n",
    "q_phi = AdaptiveMdn(num_components=4, input_dim)\n",
    "p_crit = NceRankCrit(p_theta, q_phi, J)\n",
    "q_crit = AdaptiveRankKernel(p_theta, q_phi, J)\n",
    "\n",
    "training_data = MultivariateNormalData(mu_star, cov_star, N)\n",
    "train_loader = torch.utils.data.DataLoader(training_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "adaptive_losses, adaptive_metrics = train_model_adaptive_proposal(p_theta,\n",
    "                                                        q_phi,\n",
    "                                                        p_crit,\n",
    "                                                        q_crit,\n",
    "                                                        metric,\n",
    "                                                        train_loader,\n",
    "                                                        None,\n",
    "                                                        neg_sample_size=J,\n",
    "                                                        num_epochs=num_epochs,\n",
    "                                                        stopping_condition=no_stopping,\n",
    "                                                        lr=learn_rate)\n",
    "# plot_distrs_adaptive(p_d, p_theta, q_phi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f004f620",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "iters = torch.arange(start=0, end=p_d_metrics.size(0), step=10)\n",
    "iters = torch.arange(start=0, end=2050, step=10)\n",
    "ax.plot(iters, nan_pad(p_d_metrics, num_epochs+1)[iters], label=\"$q=p_d$\")\n",
    "ax.loglog(iters, nan_pad(p_t_metrics, num_epochs+1)[iters], label=\"$q=p_{\\\\theta}$\")\n",
    "ax.loglog(iters, adaptive_metrics[iters], label=\"$q=q_{\\\\varphi}$\")\n",
    "ax.legend();\n",
    "# ax.set_xlim([0, 250])\n",
    "ax.set_title(\"Choice of proposal distribution\")\n",
    "ax.set_xlabel(\"Iter. step $t$\")\n",
    "ax.set_ylabel(\"KL$(p_d || p_{\\\\theta})$\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b2ec64",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.experiments.utils import process_plot_data\n",
    "from src.experiments.adaptive_proposal import plot_kl_div\n",
    "data = process_plot_data(torch.column_stack((p_d_metrics, p_t_metrics, adaptive_metrics)), num_epochs+1, res=1)\n",
    "\n",
    "plot_kl_div(data[:, 0], data[:, 1], data[:, 2], data[:, 3])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
