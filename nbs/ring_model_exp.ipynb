{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b21d34e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "from src.nce.binary import NceBinaryCrit\n",
    "from src.nce.cnce import CondNceCrit\n",
    "from src.nce.cd_cnce import CdCnceCrit\n",
    "\n",
    "from src.noise_distr.normal import MultivariateNormal\n",
    "from src.noise_distr.conditional_normal import ConditionalMultivariateNormal\n",
    "\n",
    "from src.models.ring_model.ring_model import RingModel, RingModelNCE, unnorm_ring_model_log_pdf\n",
    "from src.data.ring_model_dataset import RingModelDataset\n",
    "\n",
    "from src.training.model_training import train_model\n",
    "from src.training.training_utils import PrecisionErrorMetric, no_change_stopping_condition\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5bf82d0",
   "metadata": {},
   "source": [
    "## EXPERIMENT HELP FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c16bad1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Noise distribution parameters, NCE\n",
    "\n",
    "# Adapted from https://github.com/ciwanceylan/CNCE-matlab/blob/master/matlab/synthetic_data/bin/estimation/noise/continous/gNCEnoise.m\n",
    "def get_nce_noise_distr_par(y):\n",
    "    mu = torch.mean(y, dim=0)\n",
    "    cov = torch.cov(torch.transpose(y, 0, 1))\n",
    "    \n",
    "    return mu, cov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ffabfb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Noise distribution parameters, CNCE\n",
    "# Adapted from https://github.com/ciwanceylan/CNCE-matlab/blob/master/matlab/synthetic_data/bin/estimation/noise/continous/gNoise.m\n",
    "\n",
    "def get_cnce_epsilon_base(y):\n",
    "    return torch.std(y, dim=-1).mean()\n",
    "\n",
    "def get_cnce_covariance_matrix(epsilon_factor, epsilon_base, num_dims):\n",
    "    return torch.eye(num_dims) * (epsilon_factor * epsilon_base)**2\n",
    "\n",
    "\n",
    "# Adapted from https://github.com/ciwanceylan/CNCE-matlab/blob/master/matlab/synthetic_data/bin/estimation/noise/continous/gEpsilonBaseFun.m\n",
    "def get_cnce_noise_distr_par(y, neg_sample_size, mu, log_precision):\n",
    "    epsilon_factor = get_cnce_epsilon_factor(y, neg_sample_size, mu, log_precision)\n",
    "    epsilon_base = get_cnce_epsilon_base(y)\n",
    "    return get_cnce_covariance_matrix(epsilon_factor, epsilon_base, y.size(-1))\n",
    "\n",
    "\n",
    "def evaluate_cnce_loss(epsilon_factor, neg_sample_size, p_m):\n",
    "    \n",
    "    epsilon_base = get_cnce_epsilon_base(y)\n",
    "    cov_noise = get_cnce_covariance_matrix(epsilon_factor, epsilon_base, y.size(-1))\n",
    "    \n",
    "    p_n = ConditionalMultivariateNormal(cov=cov_noise)\n",
    "    criterion = CondNceCrit(p_m, p_n, neg_sample_size)\n",
    "    \n",
    "    return criterion.crit(y, 0)\n",
    "\n",
    "\n",
    "def get_cnce_epsilon_factor(y, neg_sample_size, mu, log_precision, thrs_lower=0.05, thrs_upper=0.5, inc_rate=0.2, \n",
    "                            dec_rate=0.5, max_iter=500, eps_hard_cap=1000):\n",
    "    \n",
    "    loss_zero = np.log(2) # Loss as epsilon -> 0 \n",
    "    loss_inf = 0  # Loss as epsilon -> inf\n",
    "    thrs_upper = thrs_upper * loss_zero  \n",
    "    \n",
    "    num_dims = y.size(-1)\n",
    "    p_m = RingModel(mu=mu, log_precision=log_precision)  \n",
    "    \n",
    "    epsilon_factor = 0.5  # Start value \n",
    "\n",
    "    # Calculate initial loss\n",
    "    loss = evaluate_cnce_loss(epsilon_factor, neg_sample_size, p_m)\n",
    "\n",
    "    # Iterate until conditions are met \n",
    "    k = 1;\n",
    "    while (k < max_iter) and (abs(1 - (loss/loss_zero)) < thrs_lower or loss < thrs_upper ) and (epsilon_factor < eps_hard_cap):\n",
    "\n",
    "        if abs(1 - (loss/loss_zero)) < thrs_lower: \n",
    "            epsilon_factor = (1 + inc_rate) * epsilon_factor\n",
    "        elif loss < thrs_upper:\n",
    "            epsilon_factor = (1 - dec_rate) * epsilon_factor\n",
    "   \n",
    "        loss = evaluate_cnce_loss(epsilon_factor, neg_sample_size, p_m)\n",
    "\n",
    "        k = k + 1\n",
    "    \n",
    "    return epsilon_factor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d554f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generation of true parameters, parameter initialisation\n",
    "\n",
    "# Adapted from https://github.com/ciwanceylan/CNCE-matlab/blob/master/matlab/synthetic_data/bin/data_generation/generate_parameters.m\n",
    "def generate_true_params(mu_min=5, mu_max=10, sigma_min=0.3, sigma_max=1.5):\n",
    "    mu = (mu_max - mu_min) * torch.rand(1) + mu_min\n",
    "    sigma = (sigma_max - sigma_min) * torch.rand(1) + sigma_min\n",
    "    precision = sigma**(-2)\n",
    "    z = -0.5 * torch.log(2 * torch.tensor(np.pi)) - torch.log(sigma) \n",
    "    \n",
    "    return mu, precision, z\n",
    "\n",
    "def initialise_params(mu_min=6, mu_max=8, sigma_min=0.3, sigma_max=1.5, z_min = 0.01):\n",
    "    mu = (mu_max - mu_min) * torch.rand(1) + mu_min\n",
    "    sigma = (sigma_max - sigma_min) * torch.rand(1) + sigma_min\n",
    "    precision = sigma**(-2)\n",
    "    \n",
    "    z = torch.rand(1) + z_min\n",
    "\n",
    "    return mu, torch.log(precision), torch.log(z)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b587f6",
   "metadata": {},
   "source": [
    "## NOISE DISTRIBUTIONS PLOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e49199fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some helper functions\n",
    "def get_grid_data(y_min=-10, y_max=10, grid_size=100):\n",
    "    ny = np.linspace(y_min, y_max, grid_size)\n",
    "    Y1, Y2 = np.meshgrid(ny, ny)\n",
    "    y_grid = torch.tensor(np.column_stack((Y1.reshape(-1), Y2.reshape(-1))))\n",
    "    \n",
    "    return y_grid\n",
    "\n",
    "\n",
    "def sample_nce(y, num_samples):\n",
    "    mu_noise_nce, cov_noise_nce = get_nce_noise_distr_par(y)\n",
    "\n",
    "    p_n = MultivariateNormal(mu=mu_noise_nce, cov=cov_noise_nce)\n",
    "    return p_n.sample(torch.Size((num_samples,)))\n",
    "\n",
    "\n",
    "def sample_cnce(y, num_samples, mu, log_precision, neg_sample_size=10):\n",
    "    cov_noise = get_cnce_noise_distr_par(y, neg_sample_size, mu, log_precision)\n",
    "    \n",
    "    p_n = ConditionalMultivariateNormal(cov=cov_noise)\n",
    "    \n",
    "    return p_n.sample(torch.Size((num_samples,)), y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e0b95d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2D example\n",
    "num_dims = 2\n",
    "mu, precision = 7, 1\n",
    "\n",
    "\n",
    "# Ring model pdf \n",
    "y_grid = get_grid_data()\n",
    "log_pdf = unnorm_ring_model_log_pdf(y_grid, mu=mu, precision=precision)\n",
    "\n",
    "# Sample from noise distr. based on some real training data\n",
    "num_samples = 1000\n",
    "y = RingModelDataset(sample_size=num_samples, num_dims=num_dims, mu=mu, precision=precision, \n",
    "                     root_dir=\"res/datasets/example_data\").get_full_data()\n",
    "\n",
    "# NCE \n",
    "y_samples_nce = sample_nce(y, num_samples)\n",
    "log_pdf_nce = unnorm_ring_model_log_pdf(y_samples_nce, mu=mu, precision=precision)\n",
    "\n",
    "# CNCE \n",
    "y_samples_cnce = sample_cnce(y, num_samples, mu, torch.log(torch.tensor(precision)))\n",
    "log_pdf_cnce = unnorm_ring_model_log_pdf(y_samples_cnce, mu=mu, precision=precision)\n",
    "\n",
    "\n",
    "# Visualise\n",
    "fig, ax = plt.subplots(1, 3, figsize=(12, 3))\n",
    "\n",
    "ax[0].scatter(y_grid[:, 0], y_grid[:, 1], c=log_pdf, cmap='inferno')\n",
    "ax[0].set_title(\"Ring model log pdf (unnorm.)\")\n",
    "\n",
    "ax[1].scatter(y_samples_nce[:, 0], y_samples_nce[:, 1], c=log_pdf_nce, cmap='inferno')\n",
    "ax[1].set_title(\"NCE noise distr.\")\n",
    "\n",
    "ax[2].scatter(y_samples_cnce[:, 0], y_samples_cnce[:, 1], c=log_pdf_cnce, cmap='inferno')\n",
    "ax[2].set_title(\"CNCE noise distr.\")\n",
    "            \n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4307bfe",
   "metadata": {},
   "source": [
    "## EXPERIMENTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa4eb4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data specs\n",
    "num_dims = 5\n",
    "\n",
    "# Experiments specs\n",
    "num_samples = [20, 50, 100, 200]\n",
    "num_neg_samples = [2, 5, 10]\n",
    "reps = 2\n",
    "\n",
    "# Criteria \n",
    "criteria = [CdCnceCrit, NceBinaryCrit, CondNceCrit]\n",
    "crit_labels = [\"conditional_two_steps\", \"binary\", \"conditional\"]\n",
    "\n",
    "# Training specs\n",
    "batch_size = 32\n",
    "num_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a08bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run experiments\n",
    "\n",
    "error_res = np.zeros((len(num_samples), len(num_neg_samples), len(criteria), reps))\n",
    "\n",
    "# Sorry fÃ¶r alla loopar\n",
    "for i, N in enumerate(num_samples):\n",
    "    \n",
    "    for j, J in enumerate(num_neg_samples):\n",
    "        \n",
    "        for rep in range(reps):\n",
    "            \n",
    "            # Get data \n",
    "            mu, precision, _ = generate_true_params()\n",
    "            error_metric = PrecisionErrorMetric(true_precision=precision).metric            \n",
    "                        \n",
    "            training_data = RingModelDataset(sample_size=N, num_dims=num_dims, mu=mu.numpy(), precision=precision.numpy(), \n",
    "                                             root_dir=\"res/datasets/ring_data_size_\" + str(N) + \"_nn_\" + str(J) + \"_rep_\" + str(rep))\n",
    "            train_loader = torch.utils.data.DataLoader(training_data, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "            # Initialise           \n",
    "            _, log_precision_init, log_z_init = initialise_params()\n",
    "    \n",
    "            # Get noise distr. params\n",
    "            mu_noise_nce, cov_noise_nce = get_nce_noise_distr_par(training_data.get_full_data())\n",
    "            cov_noise_cnce = get_cnce_noise_distr_par(training_data.get_full_data(), J, mu, log_precision_init)\n",
    "                        \n",
    "            for k, (crit, lab) in enumerate(zip(criteria, crit_labels)):\n",
    "                \n",
    "                # TODO: handle this differently? Dictionary?\n",
    "                if crit == NceBinaryCrit: # TODO: Nicer way to do this comparison?\n",
    "                    p_m = RingModelNCE(mu=mu, log_precision=log_precision_init, log_part_fn=log_z_init)\n",
    "                else:\n",
    "                    p_m = RingModel(mu=mu, log_precision=log_precision_init)\n",
    "                    \n",
    "                if crit == CondNceCrit or crit == CdCnceCrit:\n",
    "                    p_n = ConditionalMultivariateNormal(cov=cov_noise_cnce)\n",
    "                else:\n",
    "                    p_n = MultivariateNormal(mu=mu_noise_nce, cov=cov_noise_nce)\n",
    "                    \n",
    "                if crit == CdCnceCrit:\n",
    "                    criterion = crit(p_m, p_n, J, 2)\n",
    "                else:\n",
    "                    criterion = crit(p_m, p_n, J)\n",
    "                \n",
    "                save_dir = None\n",
    "                error_res[i, j, k, rep] = train_model(criterion, error_metric, train_loader, save_dir, neg_sample_size=J, num_epochs=num_epochs,\n",
    "                                                      stopping_condition=no_change_stopping_condition)\n",
    "\n",
    "np.save(\"res/final_param_error_ring_model_all\", error_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c0a95f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Persistent\n",
    "from src.nce.per_cnce import PersistentCondNceCrit\n",
    "N, J, rep = 20, 2, 1\n",
    "\n",
    "# Get data \n",
    "mu, precision, _ = generate_true_params()\n",
    "error_metric = PrecisionErrorMetric(true_precision=precision).metric            \n",
    "\n",
    "training_data = RingModelDataset(sample_size=N, num_dims=num_dims, mu=mu.numpy(), precision=precision.numpy(), \n",
    "                                 root_dir=\"res/datasets/ring_data_size_\" + str(N) + \"_nn_\" + str(J) + \"_rep_\" + str(rep))\n",
    "train_loader = torch.utils.data.DataLoader(training_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Initialise           \n",
    "_, log_precision_init, log_z_init = initialise_params()\n",
    "\n",
    "# Get noise distr. params\n",
    "mu_noise_nce, cov_noise_nce = get_nce_noise_distr_par(training_data.get_full_data())\n",
    "cov_noise_cnce = get_cnce_noise_distr_par(training_data.get_full_data(), J, mu, log_precision_init)\n",
    "                        \n",
    "\n",
    "p_m = RingModel(mu=mu, log_precision=log_precision_init)\n",
    "\n",
    "p_n = ConditionalMultivariateNormal(cov=cov_noise_cnce)\n",
    "\n",
    "criterion = CondNceCrit(p_m, p_n, J)\n",
    "\n",
    "lab = \"dump\"\n",
    "save_dir = \"res/param_error_\" + lab + \"_samples_\" + str(N) + \"_num_neg_\" + str(J) + \"_rep_\" + str(rep)\n",
    "err = train_model(criterion, error_metric, train_loader, save_dir, neg_sample_size=J, num_epochs=num_epochs,\n",
    "                                  stopping_condition=no_change_stopping_condition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee089e53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9fc5a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For visualising results\n",
    "def plot_res(x, error, label, col, ax):\n",
    "    ax.plot(x, np.mean(error, axis=-1), color=col, linewidth=1.0, marker='o', label=label)\n",
    "\n",
    "    ax.fill_between(x, np.min(error, axis=-1), np.max(error, axis=-1), alpha=0.1, color=col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c560bfe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise results\n",
    "\n",
    "error_res = np.load(\"res/final_param_error_ring_model_all.npy\")\n",
    "\n",
    "fig, ax = plt.subplots(len(num_neg_samples), 1, figsize=(8, 15))\n",
    "colors = ['b', 'r', 'g']\n",
    "\n",
    "log_num_samples = np.log(np.array(num_samples))\n",
    "for j, axis in enumerate(ax):\n",
    "    for k, (crit, lab) in enumerate(zip(criteria, crit_labels)):\n",
    "        plot_res(log_num_samples, np.log(error_res[:, j, k, :]), lab, colors[k], axis)\n",
    "        \n",
    "    axis.set_title(\"Num neg. samples: {}\".format(num_neg_samples[j]))    \n",
    "    axis.set_xlabel(\"Log(N)\")\n",
    "    axis.set_ylabel(\"Log(SE)\")\n",
    "    axis.legend()\n",
    "        \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ab4d8d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
