{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "006eb13f",
   "metadata": {},
   "source": [
    "# Choice of proposal distribution\n",
    "\n",
    "We investigate the effect of the proposal distribution when learning an unnormalised model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "626e8a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch\n",
    "from scipy.linalg import sqrtm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "from src.nce.cnce import CondNceCrit\n",
    "from src.nce.rank import NceRankCrit\n",
    "\n",
    "from src.noise_distr.normal import MultivariateNormal\n",
    "from src.models.gaussian_model import DiagGaussianModel\n",
    "\n",
    "from src.training.model_training import train_model, train_model_model_proposal\n",
    "from src.data.normal import MultivariateNormalData\n",
    "from src.training.training_utils import Mse, MvnKlDiv, no_change_stopping_condition, no_stopping\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "def mvn_curve(mu, cov, std=1, res=100):\n",
    "    with torch.no_grad():\n",
    "        angles = torch.linspace(0, 2*torch.pi, res)\n",
    "        curve_param = torch.column_stack((torch.cos(angles), torch.sin(angles)))\n",
    "        ellipsis = std * curve_param @ torch.Tensor(sqrtm(cov))\n",
    "        return mu + ellipsis\n",
    "    \n",
    "def plot_mvn(levels, ax, label):\n",
    "    ax.plot(levels[:, 0], levels[:, 1], label=label)\n",
    "\n",
    "def plot_distrs_ideal(p_d, p_t_d, p_t_t):    \n",
    "    fig, ax = plt.subplots()\n",
    "    ax.set_xlim([-3, 10])\n",
    "    ax.set_ylim([-3, 10])\n",
    "    distrs = [\n",
    "        (p_d.mu, p_d.cov, \"$p_{d}}$\"),\n",
    "        (p_t_d.mu, p_t_d.cov(), \"$q=p_d$\"),\n",
    "        (p_t_t.mu, p_t_t.cov(), \"$q = p_{\\\\theta}$\")\n",
    "    ]\n",
    "    for mu, cov, label in distrs:\n",
    "        plot_mvn(mvn_curve(mu, cov), ax, label)\n",
    "    ax.set_title(\"Comparison, optimal proposal distrs.\")\n",
    "    ax.legend()\n",
    "\n",
    "def plot_distrs_adaptive(p_d, p_theta, q_phi):    \n",
    "    fig, ax = plt.subplots()\n",
    "    ax.set_xlim([-3, 10])\n",
    "    ax.set_ylim([-3, 10])\n",
    "    distrs = [\n",
    "        (p_d.mu, p_d.cov, \"$p_{d}}$\"),\n",
    "        (p_theta.mu, p_theta.cov(), \"$p_{\\\\theta}$\"),\n",
    "        (q_phi.mu, q_phi.cov(), \"$q_{\\\\varphi}$\")\n",
    "    ]\n",
    "    for mu, cov, label in distrs:\n",
    "        plot_mvn(mvn_curve(mu, cov), ax, label)\n",
    "    ax.set_title(\"Adaptive proposal\")\n",
    "    ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f85269ee",
   "metadata": {},
   "source": [
    "# Idealistic case\n",
    "\n",
    "Assume that we can evaluate and sample from $p_d$ and $p_\\theta$,\n",
    "which is the better alternative as the proposal distribution $q$? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ccd2c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "D, N, J = 2, 100, 10 # Dimension, Num. data samples, Num neg. samples\n",
    "# Data model\n",
    "mu_star, cov_star = torch.ones(D,), torch.eye(D)\n",
    "p_d = MultivariateNormal(mu_star, cov_star)\n",
    "\n",
    "init_mu, init_cov =5.0*torch.ones(D,), 1*torch.eye(D)\n",
    "\n",
    "num_epochs = 1000\n",
    "batch_size = 20\n",
    "learn_rate = 0.01*batch_size**0.5\n",
    "\n",
    "training_data = MultivariateNormalData(mu_star, cov_star, N)\n",
    "train_loader = torch.utils.data.DataLoader(training_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Metrics\n",
    "kl_div = MvnKlDiv(p_d.mu, p_d.cov).metric\n",
    "mse = Mse(p_d.mu).metric\n",
    "metric = kl_div\n",
    "\n",
    "# q = p_d\n",
    "p_t_data_noise = DiagGaussianModel(init_mu.clone(), init_cov.clone())\n",
    "plot_distrs_ideal(p_d, p_t_data_noise, p_t_model_noise)\n",
    "criterion = NceRankCrit(p_t_data_noise, p_d, J)\n",
    "p_d_losses, p_d_metrics = train_model(criterion,\n",
    "                  metric,\n",
    "                  train_loader,\n",
    "                  None,\n",
    "                  neg_sample_size=J,\n",
    "                  num_epochs=num_epochs,\n",
    "                  stopping_condition=no_change_stopping_condition,\n",
    "                  lr=learn_rate)\n",
    "\n",
    "# q = p_theta\n",
    "p_t_model_noise = DiagGaussianModel(init_mu.clone(), init_cov.clone())\n",
    "p_t_losses, p_t_metrics = train_model_model_proposal(p_t_model_noise,\n",
    "                           NceRankCrit,\n",
    "                           metric,\n",
    "                           train_loader,\n",
    "                           None,\n",
    "                           J,\n",
    "                           num_epochs,\n",
    "                           lr=learn_rate)\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(torch.arange(p_d_metrics.size(0)), p_d_metrics, label=\"$q=p_d$\")\n",
    "ax.plot(torch.arange(p_t_metrics.size(0)), p_t_metrics, label=\"$q=p_{\\\\theta}$\")\n",
    "ax.legend();\n",
    "ax.set_title(\"Choice of proposal distribution\")\n",
    "ax.set_xlabel(\"Iter. step $t$\")\n",
    "ax.set_ylabel(\"KL$(p_d || p_{\\\\theta})$\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae63a3f",
   "metadata": {},
   "source": [
    "# Adaptive proposal\n",
    "\n",
    "Assume that we have a learnable proposal $q_\\varphi$.\n",
    "We jointly learn this proposal by minimising\n",
    "$$\n",
    "KL(p_\\theta \\| q_\\varphi) \\propto - \\mathbb{E}_{x \\sim p_\\theta} \\log q_\\varphi(x) = \\mathcal{L}_\\varphi\n",
    "$$\n",
    "with,\n",
    "$$\n",
    "\\nabla_\\varphi \\mathcal{L}_\\varphi \\approx \\sum_{j=0}^J w(x_j) \\nabla \\log q_\\varphi(x_j)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e44b2b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.nce.adaptive_rank import AdaptiveRankKernel\n",
    "from src.noise_distr.adaptive import AdaptiveDiagGaussianModel\n",
    "\n",
    "\n",
    "def train_model_adaptive_proposal(\n",
    "    p_theta,\n",
    "    q_phi,\n",
    "    p_criterion,\n",
    "    q_criterion,\n",
    "    evaluation_metric,\n",
    "    train_loader,\n",
    "    save_dir,\n",
    "    neg_sample_size,\n",
    "    num_epochs,\n",
    "    stopping_condition=no_stopping,\n",
    "    lr: float = 0.1,\n",
    "):\n",
    "    \"\"\"Training loop for adaptive proposal q_phi\n",
    "\n",
    "    Training loop for jointly learning p_tilde_theta and q_phi.\n",
    "    Where we assume that we can sample and evaluate q_phi.\n",
    "    \"\"\"\n",
    "    p_optimizer = torch.optim.SGD(p_theta.parameters(), lr=lr)\n",
    "    q_optimizer = torch.optim.SGD(q_phi.parameters(), lr=lr)\n",
    "    batch_metrics = []\n",
    "    batch_metrics.append(evaluation_metric(q_phi))\n",
    "    batch_losses = []\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        # print(f\"Epoch {epoch}\")\n",
    "        old_params = torch.nn.utils.parameters_to_vector(q_phi.parameters())\n",
    "        for _, (y, idx) in enumerate(train_loader, 0):\n",
    "\n",
    "            #with torch.no_grad():\n",
    "            #    p_loss = p_criterion.crit(y, None)\n",
    "            #    batch_losses.append(p_loss.item())\n",
    "            # Calculate and assign gradients\n",
    "            p_optimizer.zero_grad()\n",
    "            p_crit.calculate_crit_grad(y, idx)\n",
    "            p_optimizer.step()\n",
    "\n",
    "            q_optimizer.zero_grad()\n",
    "            q_crit.calculate_crit_grad(y, idx)\n",
    "            q_optimizer.step()\n",
    "            \n",
    "            batch_metrics.append(evaluation_metric(q_phi))\n",
    "            \n",
    "        if stopping_condition(\n",
    "            torch.nn.utils.parameters_to_vector(q_phi.parameters()), old_params\n",
    "        ):\n",
    "            print(\"Training converged\")\n",
    "            break\n",
    "    return torch.tensor(batch_losses), torch.tensor(batch_metrics)\n",
    "\n",
    "D, N, J = 2, 1000, 50 # Dimension, Num. data samples, Num neg. samples\n",
    "# Data model\n",
    "mu_star, cov_star = 3*torch.ones(D,), 1*torch.eye(D)\n",
    "p_d = MultivariateNormal(mu_star, cov_star)\n",
    "\n",
    "init_mu, init_cov = 1.0 * torch.ones(D,), 1 * torch.eye(D)\n",
    "p_theta = DiagGaussianModel(init_mu.clone(), init_cov.clone())\n",
    "q_phi = AdaptiveDiagGaussianModel(-1.0 * init_mu.clone(), 2.0*init_cov.clone())\n",
    "p_crit = NceRankCrit(p_theta, q_phi, J)\n",
    "q_crit = AdaptiveRankKernel(p_theta, q_phi, J)\n",
    "\n",
    "num_epochs = 200\n",
    "batch_size = N\n",
    "learn_rate = 0.01*batch_size**0.5\n",
    "\n",
    "training_data = MultivariateNormalData(mu_star, cov_star, N)\n",
    "train_loader = torch.utils.data.DataLoader(training_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Metrics\n",
    "kl_div = MvnKlDiv(p_d.mu, p_d.cov).metric\n",
    "mse = Mse(p_d.mu).metric\n",
    "metric = kl_div\n",
    "\n",
    "# q = p_d\n",
    "\n",
    "p_d_losses, p_d_metrics = train_model_adaptive_proposal(p_theta,\n",
    "                                                        q_phi,\n",
    "                                                        p_crit,\n",
    "                                                        q_crit,\n",
    "                                                        metric,\n",
    "                                                        train_loader,\n",
    "                                                        None,\n",
    "                                                        neg_sample_size=J,\n",
    "                                                        num_epochs=num_epochs,\n",
    "                                                        stopping_condition=no_stopping,\n",
    "                                                        lr=learn_rate)\n",
    "plot_distrs_adaptive(p_d, p_theta, q_phi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3caaee93",
   "metadata": {},
   "source": [
    "Gradient: $q_\\phi = p_\\theta$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c323a6a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from src.part_fn_utils import concat_samples, norm_weights\n",
    "N, J, D = 1, 3, 2\n",
    "pi = np.pi\n",
    "mu, cov = torch.zeros(D,), torch.eye(D)\n",
    "p_theta = AdaptiveDiagGaussianModel(mu, cov)\n",
    "q_phi = AdaptiveDiagGaussianModel(mu, cov)\n",
    "q_crit = AdaptiveRankKernel(p_theta, q_phi, J)\n",
    "y = torch.zeros((N, D))\n",
    "y_samples = torch.zeros((N, J, D))\n",
    "# y_samples = q_crit.sample_noise(J, y)\n",
    "ys = concat_samples(y, y_samples)\n",
    "# print(f\"Init probs: p={p_theta.prob(ys)} q={q_phi.prob(ys)}\")\n",
    "ws = norm_weights(q_crit._unnorm_w(y, y_samples))\n",
    "diag_deriv = 2 * torch.ones(D,)\n",
    "exact_grad(ys, ws, mu, cov), q_phi.grad_log_prob(ys, ws)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3bdb4c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def exact_grad(ys, ws, mu, cov):\n",
    "    diff = (ys - mu).reshape((N+J, D)).T\n",
    "    print(diff.shape)\n",
    "    cov_inv = torch.linalg.inv(cov)\n",
    "    emp_cov = (ws * diff) @ diff.T\n",
    "    mu_deriv = (cov_inv @ diff) @ ws.T\n",
    "    cov_deriv = - cov_inv / 2 + cov_inv @ emp_cov @ cov_inv / 2\n",
    "    return mu_deriv.T, cov_deriv\n",
    "ys.shape, mu.shape\n",
    "diff = ys.reshape((N+J, D)).T - mu\n",
    "diff.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
