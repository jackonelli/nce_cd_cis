{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "006eb13f",
   "metadata": {},
   "source": [
    "# Choice of proposal distribution\n",
    "\n",
    "We investigate the effect of the proposal distribution when learning an unnormalised model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "626e8a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from functools import partial\n",
    "import torch\n",
    "from scipy.linalg import sqrtm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "from src.nce.cnce import CondNceCrit\n",
    "from src.nce.rank import NceRankCrit\n",
    "\n",
    "from src.noise_distr.normal import MultivariateNormal\n",
    "from src.models.gaussian_model import DiagGaussianModel\n",
    "\n",
    "from src.training.model_training import train_model, train_model_model_proposal\n",
    "from src.data.normal import MultivariateNormalData\n",
    "from src.training.training_utils import Mse, MvnKlDiv, no_change_stopping_condition, no_stopping\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "def mvn_curve(mu, cov, std=1, res=100):\n",
    "    with torch.no_grad():\n",
    "        angles = torch.linspace(0, 2*torch.pi, res)\n",
    "        curve_param = torch.column_stack((torch.cos(angles), torch.sin(angles)))\n",
    "        ellipsis = std * curve_param @ torch.Tensor(sqrtm(cov))\n",
    "        return mu + ellipsis\n",
    "    \n",
    "def plot_mvn(levels, ax, label):\n",
    "    ax.plot(levels[:, 0], levels[:, 1], label=label)\n",
    "\n",
    "def plot_distrs_ideal(p_d, p_t_d, p_t_t):    \n",
    "    fig, ax = plt.subplots()\n",
    "    ax.set_xlim([-3, 10])\n",
    "    ax.set_ylim([-3, 10])\n",
    "    distrs = [\n",
    "        (p_d.mu, p_d.cov, \"$p_{d}}$\"),\n",
    "        (p_t_d.mu, p_t_d.cov(), \"$q=p_d$\"),\n",
    "        (p_t_t.mu, p_t_t.cov(), \"$q = p_{\\\\theta}$\")\n",
    "    ]\n",
    "    for mu, cov, label in distrs:\n",
    "        plot_mvn(mvn_curve(mu, cov), ax, label)\n",
    "    ax.set_title(\"Comparison, optimal proposal distrs.\")\n",
    "    ax.legend()\n",
    "\n",
    "def plot_distrs_adaptive(p_d, p_theta, q_phi):    \n",
    "    fig, ax = plt.subplots()\n",
    "    ax.set_xlim([-3, 10])\n",
    "    ax.set_ylim([-3, 10])\n",
    "    distrs = [\n",
    "        (p_d.mu, p_d.cov, \"$p_{d}}$\"),\n",
    "        (p_theta.mu, p_theta.cov(), \"$p_{\\\\theta}$\"),\n",
    "        (q_phi.mu, q_phi.cov(), \"$q_{\\\\varphi}$\")\n",
    "    ]\n",
    "    for mu, cov, label in distrs:\n",
    "        plot_mvn(mvn_curve(mu, cov), ax, label)\n",
    "    ax.set_title(\"Adaptive proposal\")\n",
    "    ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68cebd02",
   "metadata": {},
   "source": [
    "# Common setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2720390",
   "metadata": {},
   "outputs": [],
   "source": [
    "D, N, J = 5, 100, 10 # Dimension, Num. data samples, Num neg. samples\n",
    "mu_star, cov_star = torch.ones(D,), torch.eye(D)\n",
    "\n",
    "# Data distribution\n",
    "p_d = MultivariateNormal(mu_star, cov_star)\n",
    "# Model distribution\n",
    "init_mu, init_cov =5.0*torch.ones(D,), 4.0*torch.eye(D)\n",
    "\n",
    "# Optimisation\n",
    "num_epochs = 100\n",
    "batch_size = N\n",
    "learn_rate = 0.05*batch_size**0.5\n",
    "scheduler_opts = (30, 0.9)\n",
    "\n",
    "# Metrics\n",
    "kl_div = MvnKlDiv(p_d.mu, p_d.cov).metric\n",
    "mse = Mse(p_d.mu).metric\n",
    "metric = kl_div\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f85269ee",
   "metadata": {},
   "source": [
    "# Idealistic case\n",
    "\n",
    "Assuming that we can evaluate and sample from $p_d$ and $p_\\theta$,\n",
    "which is the better alternative as the proposal distribution $q$? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ccd2c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# q = p_theta\n",
    "p_t_model_noise = DiagGaussianModel(init_mu.clone(), init_cov.clone())\n",
    "print(\"Training with q = p_theta\")\n",
    "\n",
    "training_data = MultivariateNormalData(mu_star, cov_star, N)\n",
    "train_loader = torch.utils.data.DataLoader(training_data, batch_size=batch_size, shuffle=True)\n",
    "p_t_losses, p_t_metrics = train_model_model_proposal(p_t_model_noise,\n",
    "                                                     NceRankCrit,\n",
    "                                                     metric,\n",
    "                                                     train_loader,\n",
    "                                                     None,\n",
    "                                                     J,\n",
    "                                                     num_epochs,\n",
    "                                                     # stopping_condition=no_change_stopping_condition,\n",
    "                                                     stopping_condition=no_stopping,\n",
    "                                                     lr=learn_rate,\n",
    "                                                     scheduler_opts=scheduler_opts\n",
    "                                                    )\n",
    "\n",
    "# q = p_d\n",
    "p_t_data_noise = DiagGaussianModel(init_mu.clone(), init_cov.clone())\n",
    "criterion = NceRankCrit(p_t_data_noise, p_d, J)\n",
    "print(\"Training with q = p_d\")\n",
    "\n",
    "training_data = MultivariateNormalData(mu_star, cov_star, N)\n",
    "train_loader = torch.utils.data.DataLoader(training_data, batch_size=batch_size, shuffle=True)\n",
    "p_d_losses, p_d_metrics = train_model(criterion,\n",
    "                                      metric,\n",
    "                                      train_loader,\n",
    "                                      None,\n",
    "                                      neg_sample_size=J,\n",
    "                                      num_epochs=num_epochs,\n",
    "                                      # stopping_condition=no_change_stopping_condition,\n",
    "                                      stopping_condition=no_stopping,\n",
    "                                      lr=learn_rate,\n",
    "                                      scheduler_opts=scheduler_opts\n",
    "                                     )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae63a3f",
   "metadata": {},
   "source": [
    "# Adaptive proposal\n",
    "\n",
    "Assume that we have a learnable proposal $q_\\varphi$.\n",
    "We jointly learn this proposal by minimising\n",
    "$$\n",
    "KL(p_\\theta \\| q_\\varphi) \\propto - \\mathbb{E}_{x \\sim p_\\theta} \\log q_\\varphi(x) = \\mathcal{L}_\\varphi\n",
    "$$\n",
    "with,\n",
    "$$\n",
    "\\nabla_\\varphi \\mathcal{L}_\\varphi \\approx - \\sum_{j=0}^J w(x_j) \\nabla \\log q_\\varphi(x_j)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e44b2b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.nce.adaptive_rank import AdaptiveRankKernel\n",
    "from src.noise_distr.adaptive import AdaptiveDiagGaussianModel\n",
    "\n",
    "\n",
    "def train_model_adaptive_proposal(\n",
    "    p_theta,\n",
    "    q_phi,\n",
    "    p_criterion,\n",
    "    q_criterion,\n",
    "    evaluation_metric,\n",
    "    train_loader,\n",
    "    save_dir,\n",
    "    neg_sample_size,\n",
    "    num_epochs,\n",
    "    stopping_condition=no_stopping,\n",
    "    lr: float = 0.1,\n",
    "):\n",
    "    \"\"\"Training loop for adaptive proposal q_phi\n",
    "\n",
    "    Training loop for jointly learning p_tilde_theta and q_phi.\n",
    "    Where we assume that we can sample and evaluate q_phi.\n",
    "    \"\"\"\n",
    "    p_optimizer = torch.optim.SGD(p_theta.parameters(), lr=lr)\n",
    "    q_optimizer = torch.optim.SGD(q_phi.parameters(), lr=lr)\n",
    "    batch_metrics = []\n",
    "    batch_metrics.append(evaluation_metric(p_theta))\n",
    "    batch_losses = []\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        # print(f\"Epoch {epoch}\")\n",
    "        old_params = torch.nn.utils.parameters_to_vector(q_phi.parameters())\n",
    "        for _, (y, idx) in enumerate(train_loader, 0):\n",
    "\n",
    "            #with torch.no_grad():\n",
    "            #    p_loss = p_criterion.crit(y, None)\n",
    "            #    batch_losses.append(p_loss.item())\n",
    "            # Calculate and assign gradients\n",
    "            p_optimizer.zero_grad()\n",
    "            p_criterion.calculate_crit_grad(y, idx)\n",
    "            p_optimizer.step()\n",
    "\n",
    "            q_optimizer.zero_grad()\n",
    "            q_criterion.calculate_crit_grad(y, idx)\n",
    "            q_optimizer.step()\n",
    "            with torch.no_grad():\n",
    "                batch_metrics.append(evaluation_metric(p_theta))\n",
    "            \n",
    "        if stopping_condition(\n",
    "            torch.nn.utils.parameters_to_vector(q_phi.parameters()), old_params\n",
    "        ):\n",
    "            print(\"Training converged\")\n",
    "            break\n",
    "    return torch.tensor(batch_losses), torch.tensor(batch_metrics)\n",
    "\n",
    "p_theta = DiagGaussianModel(init_mu.clone(), init_cov.clone())\n",
    "q_phi = AdaptiveDiagGaussianModel(mu_star.clone(), cov_star.clone())\n",
    "p_crit = NceRankCrit(p_theta, q_phi, J)\n",
    "q_crit = AdaptiveRankKernel(p_theta, q_phi, J)\n",
    "\n",
    "training_data = MultivariateNormalData(mu_star, cov_star, N)\n",
    "train_loader = torch.utils.data.DataLoader(training_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "adaptive_losses, adaptive_metrics = train_model_adaptive_proposal(p_theta,\n",
    "                                                        q_phi,\n",
    "                                                        p_crit,\n",
    "                                                        q_crit,\n",
    "                                                        metric,\n",
    "                                                        train_loader,\n",
    "                                                        None,\n",
    "                                                        neg_sample_size=J,\n",
    "                                                        num_epochs=num_epochs,\n",
    "                                                        stopping_condition=no_stopping,\n",
    "                                                        lr=learn_rate)\n",
    "# plot_distrs_adaptive(p_d, p_theta, q_phi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc03387",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import Tensor\n",
    "def nan_pad(data: Tensor, length: int):\n",
    "    \"\"\"Pad tensor with NaN\n",
    "    \n",
    "    Args:\n",
    "        data: shape (N, )\n",
    "        length: length >= N\n",
    "    \"\"\"\n",
    "    assert data.dim() == 1, \"Expects 1D array, with shape (N, )\"\n",
    "    N = data.size(0)\n",
    "    print(N)\n",
    "    assert length >= N, \"Padding length must be larger than data length\"\n",
    "    padded = torch.empty((length, ))\n",
    "    padded[:N] = data\n",
    "    padded[N:] = torch.nan\n",
    "    return padded\n",
    "padded = nan_pad(p_d_metrics, num_epochs+1)\n",
    "N = p_d_metrics.size(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e3543c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "iters = torch.arange(start=0, end=p_d_metrics.size(0), step=10)\n",
    "iters = torch.arange(start=0, end=2050, step=10)\n",
    "ax.plot(iters, nan_pad(p_d_metrics, num_epochs+1)[iters], label=\"$q=p_d$\")\n",
    "ax.loglog(iters, nan_pad(p_t_metrics, num_epochs+1)[iters], label=\"$q=p_{\\\\theta}$\")\n",
    "ax.loglog(iters, adaptive_metrics[iters], label=\"$q=q_{\\\\varphi}$\")\n",
    "ax.legend();\n",
    "# ax.set_xlim([0, 250])\n",
    "ax.set_title(\"Choice of proposal distribution\")\n",
    "ax.set_xlabel(\"Iter. step $t$\")\n",
    "ax.set_ylabel(\"KL$(p_d || p_{\\\\theta})$\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "097d84e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.experiments.utils import process_plot_data\n",
    "from src.experiments.adaptive_proposal import plot_kl_div\n",
    "data = process_plot_data(torch.column_stack((p_d_metrics, p_t_metrics, adaptive_metrics)), num_epochs+1, res=1)\n",
    "\n",
    "plot_kl_div(data[:, 0], data[:, 1], data[:, 2], data[:, 3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a3dc47a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from src.experiments.utils import generate_bounds, table_data, format_table, skip_list_item\n",
    "\n",
    "end_iter = 6500\n",
    "p_d = torch.load(Path.cwd().parent / \"fig/20_runs_kl_raw_p_d.pth\").numpy()[:, :end_iter]\n",
    "p_t = torch.load(Path.cwd().parent / \"fig/20_runs_kl_raw_p_t.pth\").numpy()[:, :end_iter]\n",
    "q_f = torch.load(Path.cwd().parent / \"fig/20_runs_kl_raw_q_f.pth\").numpy()[:, :end_iter]\n",
    "num_runs = p_d.shape[0]\n",
    "\n",
    "p_d_md, p_d_lower, p_d_upper = generate_bounds(p_d)\n",
    "p_t_md, p_t_lower, p_t_upper = generate_bounds(p_t)\n",
    "q_f_md, q_f_lower, q_f_upper = generate_bounds(q_f)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f54f49fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ax = plt.subplots()\n",
    "for data, name in [(p_d, \"p_d\"), (p_t, \"p_t\"), (q_f, \"q_f\")]:\n",
    "    tmp = table_data(*generate_bounds(data))\n",
    "    it, med, low, upp = tuple(map(lambda x: skip_list_item(x, nth=30), tmp))\n",
    "\n",
    "    ax.errorbar(it, med, [low, upp], label=f\"$q={name}$\")\n",
    "ax.legend()\n",
    "# ax.set_xlim([1, end_iter + 1])\n",
    "ax.set_title(\"Choice of proposal distribution\")\n",
    "ax.set_xlabel(\"Iter. step $t$\")\n",
    "ax.set_ylabel(\"KL$(p_d || p_{\\\\theta})$\")\n",
    "ax.set_xscale(\"log\")\n",
    "ax.set_yscale(\"log\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24baa5ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "for data, name in [(p_d, \"p_d\"), (p_t, \"p_t\"), (q_f, \"q_f\")]:\n",
    "    with open(f\"{num_runs}_runs_{name}.txt\", \"w\") as f:\n",
    "        tmp = table_data(*generate_bounds(data))\n",
    "        tmp = tuple(map(lambda x: skip_list_item(x, nth=30), tmp))\n",
    "        tbl = format_table(*tmp, [\"t\",\"kl\",\"low\",\"upp\"])\n",
    "        f.writelines(tbl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72a7f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = table_data(*generate_bounds(data))\n",
    "it, med, low, upp = tuple(map(lambda x: skip_list_item(x, nth=20), tmp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b68f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "cis_kl = np.load(\"../../ebms_proposals/cis_kl_store.npy\")\n",
    "is_kl = np.load(\"../../ebms_proposals/is_kl_store.npy\")\n",
    "is_kl_nan_filter = np.delete(is_kl, (10), axis=0)\n",
    "_, ax = plt.subplots()\n",
    "for data, name in [(is_kl_nan_filter, \"IS\"), (cis_kl, \"CIS\")]:\n",
    "    it, med, low, upp = table_data(*generate_bounds(data))\n",
    "    ax.errorbar(it, med, [low, upp], label=f\"${name}$\")\n",
    "ax.legend()\n",
    "# ax.set_xlim([1, end_iter + 1])\n",
    "ax.set_title(\"Approximate KL div.\")\n",
    "ax.set_xlabel(\"Epoch\")\n",
    "ax.set_ylabel(\"KL$(p_d || p_{\\\\theta})$\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d120cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from src.experiments.utils import generate_bounds, table_data, format_table, skip_list_item\n",
    "\n",
    "runs, epoch = 5, 20\n",
    "res_dir = Path(f\"../../ebms_proposals/1dregression_1/results/{runs}_runs_{epoch}_epochs\")\n",
    "cis_kl = np.load(res_dir / \"cis_kl_eval.npy\")\n",
    "is_kl = np.load(res_dir / \"is_kl_eval.npy\")\n",
    "pcis_kl = np.load(res_dir.parent / \"5_runs_20_epochs\" / \"pcis_kl_eval.npy\")\n",
    "\n",
    "\n",
    "cis_q_nll = np.load(res_dir / \"cis_q_nll.npy\")\n",
    "is_q_nll = np.load(res_dir / \"is_q_nll.npy\")\n",
    "pcis_q_nll = np.load(res_dir.parent / \"5_runs_20_epochs\" / \"pcis_q_nll.npy\")\n",
    "\n",
    "# cis_kl = np.delete(cis_kl, (15), axis=0)\n",
    "_, (ax_kl, ax_nll) = plt.subplots(1, 2)\n",
    "vals = [\n",
    "    # (pcis_kl, \"P-CIS\"),\n",
    "    (cis_kl, \"CIS\"),\n",
    "    (is_kl, \"IS\")\n",
    "]\n",
    "for data, name in vals:\n",
    "    it, med, low, upp = table_data(*generate_bounds(data))\n",
    "    ax_kl.errorbar(it, med, [low, upp], label=f\"${name}$\")\n",
    "    # mean, std = data.mean(axis=0), data.std(axis=0)\n",
    "    # it = np.arange(1, len(mean)+1)\n",
    "    # ax_kl.errorbar(it, mean, std, label=f\"${name}$\")\n",
    "ax_kl.legend()\n",
    "ax_kl.set_title(\"Approximate KL div.\")\n",
    "ax_kl.set_xlabel(\"Epoch\")\n",
    "ax_kl.set_ylabel(\"KL$(p_d || p_{\\\\theta})$\")\n",
    "\n",
    "vals = [\n",
    "    # (pcis_q_nll, \"P-CIS\"),\n",
    "    (cis_q_nll, \"CIS\"),\n",
    "    (is_q_nll, \"IS\")\n",
    "] \n",
    "for data, name in vals:\n",
    "    it, med, low, upp = table_data(*generate_bounds(data))\n",
    "    ax_nll.errorbar(it, med, [low, upp], label=f\"${name}$\")\n",
    "    # mean, std = data.mean(axis=0), data.std(axis=0)\n",
    "    # it = np.arange(1, len(mean)+1)\n",
    "    # ax_nll.errorbar(it, mean, std, label=f\"${name}$\")\n",
    "ax_nll.legend()\n",
    "ax_nll.set_title(\"NLL: q\")\n",
    "ax_nll.set_xlabel(\"Epoch\")\n",
    "ax_nll.set_ylabel(\"NLL q\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8176c35e",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argwhere(np.isnan(cis_kl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d04347",
   "metadata": {},
   "outputs": [],
   "source": [
    "it, med, low, upp = table_data(*generate_bounds(data))\n",
    "upp.shape\n",
    "name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e5f7bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "def get_y_p(ys, xs, y_pers_dict):\n",
    "    y_ps = torch.empty(ys.size())\n",
    "    for ind, x in enumerate(xs):\n",
    "        x = x.item()\n",
    "        tmp = y_pers_dict.get(x)\n",
    "        if tmp is None:\n",
    "            y_pers_dict[x] = ys[ind].item()\n",
    "            y_ps[ind] = ys[ind]\n",
    "        else:\n",
    "            y_ps[ind] = tmp\n",
    "    return y_ps\n",
    "\n",
    "def update_y_pers(xs, y_p_J, w_tilde, y_pers_dict):\n",
    "    distr_ = torch.distributions.categorical.Categorical(logits=w_tilde)\n",
    "    smpl = distr_.sample()\n",
    "    for ind, x in enumerate(xs):\n",
    "        p_ind = smpl[ind]\n",
    "        y_pers_dict[xs[ind].item()] = y_p_J[ind, p_ind].item()\n",
    "\n",
    "\n",
    "\n",
    "B, J = 3, 4\n",
    "xs, ys = torch.arange(0, B).reshape((B, 1)), torch.ones(B, 1)\n",
    "\n",
    "y_pers_dict = dict()\n",
    "y_p = get_y_p(ys, xs, y_pers_dict)\n",
    "print(y_pers_dict)\n",
    "\n",
    "y_p_J = torch.ones(B, J) * torch.arange(0, J)\n",
    "w_tilde = torch.ones(B, J)\n",
    "update_y_pers(xs, y_p_J, w_tilde, y_pers_dict)\n",
    "y_pers_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "987a8792",
   "metadata": {},
   "outputs": [],
   "source": [
    "smpl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c596e30f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
