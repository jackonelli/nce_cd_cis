{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "006eb13f",
   "metadata": {},
   "source": [
    "# Choice of proposal distribution\n",
    "\n",
    "We investigate the effect of the proposal distribution when learning an unnormalised model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "626e8a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch\n",
    "from scipy.linalg import sqrtm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "from src.nce.cnce import CondNceCrit\n",
    "from src.nce.rank import NceRankCrit\n",
    "\n",
    "from src.noise_distr.normal import MultivariateNormal\n",
    "from src.models.gaussian_model import DiagGaussianModel\n",
    "\n",
    "from src.training.model_training import train_model, train_model_model_proposal\n",
    "from src.data.normal import MultivariateNormalData\n",
    "from src.training.training_utils import Mse, MvnKlDiv, no_change_stopping_condition, no_stopping\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "def mvn_curve(mu, cov, std=1, res=100):\n",
    "    with torch.no_grad():\n",
    "        angles = torch.linspace(0, 2*torch.pi, res)\n",
    "        curve_param = torch.column_stack((torch.cos(angles), torch.sin(angles)))\n",
    "        ellipsis = std * curve_param @ torch.Tensor(sqrtm(cov))\n",
    "        return mu + ellipsis\n",
    "    \n",
    "def plot_mvn(levels, ax, label):\n",
    "    ax.plot(levels[:, 0], levels[:, 1], label=label)\n",
    "\n",
    "def plot_distrs_ideal(p_d, p_t_d, p_t_t):    \n",
    "    fig, ax = plt.subplots()\n",
    "    ax.set_xlim([-3, 10])\n",
    "    ax.set_ylim([-3, 10])\n",
    "    distrs = [\n",
    "        (p_d.mu, p_d.cov, \"$p_{d}}$\"),\n",
    "        (p_t_d.mu, p_t_d.cov(), \"$q=p_d$\"),\n",
    "        (p_t_t.mu, p_t_t.cov(), \"$q = p_{\\\\theta}$\")\n",
    "    ]\n",
    "    for mu, cov, label in distrs:\n",
    "        plot_mvn(mvn_curve(mu, cov), ax, label)\n",
    "    ax.set_title(\"Comparison, optimal proposal distrs.\")\n",
    "    ax.legend()\n",
    "\n",
    "def plot_distrs_adaptive(p_d, p_theta, q_phi):    \n",
    "    fig, ax = plt.subplots()\n",
    "    ax.set_xlim([-3, 10])\n",
    "    ax.set_ylim([-3, 10])\n",
    "    distrs = [\n",
    "        (p_d.mu, p_d.cov, \"$p_{d}}$\"),\n",
    "        (p_theta.mu, p_theta.cov(), \"$p_{\\\\theta}$\"),\n",
    "        (q_phi.mu, q_phi.cov(), \"$q_{\\\\varphi}$\")\n",
    "    ]\n",
    "    for mu, cov, label in distrs:\n",
    "        plot_mvn(mvn_curve(mu, cov), ax, label)\n",
    "    ax.set_title(\"Adaptive proposal\")\n",
    "    ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2581688d",
   "metadata": {},
   "source": [
    "# Common setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c707e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "D, N, J = 5, 100, 10 # Dimension, Num. data samples, Num neg. samples\n",
    "mu_star, cov_star = torch.ones(D,), torch.eye(D)\n",
    "\n",
    "# Data distribution\n",
    "p_d = MultivariateNormal(mu_star, cov_star)\n",
    "# Model distribution\n",
    "init_mu, init_cov =5.0*torch.ones(D,), 4.0*torch.eye(D)\n",
    "\n",
    "# Optimisation\n",
    "num_epochs = 2500\n",
    "batch_size = N\n",
    "learn_rate = 0.01*batch_size**0.5\n",
    "\n",
    "# Metrics\n",
    "kl_div = MvnKlDiv(p_d.mu, p_d.cov).metric\n",
    "mse = Mse(p_d.mu).metric\n",
    "metric = kl_div\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f85269ee",
   "metadata": {},
   "source": [
    "# Idealistic case\n",
    "\n",
    "Assuming that we can evaluate and sample from $p_d$ and $p_\\theta$,\n",
    "which is the better alternative as the proposal distribution $q$? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ccd2c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# q = p_d\n",
    "p_t_data_noise = DiagGaussianModel(init_mu.clone(), init_cov.clone())\n",
    "criterion = NceRankCrit(p_t_data_noise, p_d, J)\n",
    "print(\"Training with q = p_d\")\n",
    "\n",
    "training_data = MultivariateNormalData(mu_star, cov_star, N)\n",
    "train_loader = torch.utils.data.DataLoader(training_data, batch_size=batch_size, shuffle=True)\n",
    "p_d_losses, p_d_metrics = train_model(criterion,\n",
    "                  metric,\n",
    "                  train_loader,\n",
    "                  None,\n",
    "                  neg_sample_size=J,\n",
    "                  num_epochs=num_epochs,\n",
    "                  stopping_condition=no_change_stopping_condition,\n",
    "                  lr=learn_rate)\n",
    "# q = p_theta\n",
    "p_t_model_noise = DiagGaussianModel(init_mu.clone(), init_cov.clone())\n",
    "print(\"Training with q = p_theta\")\n",
    "\n",
    "training_data = MultivariateNormalData(mu_star, cov_star, N)\n",
    "train_loader = torch.utils.data.DataLoader(training_data, batch_size=batch_size, shuffle=True)\n",
    "p_t_losses, p_t_metrics = train_model_model_proposal(p_t_model_noise,\n",
    "                           NceRankCrit,\n",
    "                           metric,\n",
    "                           train_loader,\n",
    "                           None,\n",
    "                           J,\n",
    "                           num_epochs,\n",
    "                           lr=learn_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae63a3f",
   "metadata": {},
   "source": [
    "# Adaptive proposal\n",
    "\n",
    "Assume that we have a learnable proposal $q_\\varphi$.\n",
    "We jointly learn this proposal by minimising\n",
    "$$\n",
    "KL(p_\\theta \\| q_\\varphi) \\propto - \\mathbb{E}_{x \\sim p_\\theta} \\log q_\\varphi(x) = \\mathcal{L}_\\varphi\n",
    "$$\n",
    "with,\n",
    "$$\n",
    "\\nabla_\\varphi \\mathcal{L}_\\varphi \\approx - \\sum_{j=0}^J w(x_j) \\nabla \\log q_\\varphi(x_j)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e44b2b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.nce.adaptive_rank import AdaptiveRankKernel\n",
    "from src.noise_distr.adaptive import AdaptiveDiagGaussianModel\n",
    "\n",
    "\n",
    "def train_model_adaptive_proposal(\n",
    "    p_theta,\n",
    "    q_phi,\n",
    "    p_criterion,\n",
    "    q_criterion,\n",
    "    evaluation_metric,\n",
    "    train_loader,\n",
    "    save_dir,\n",
    "    neg_sample_size,\n",
    "    num_epochs,\n",
    "    stopping_condition=no_stopping,\n",
    "    lr: float = 0.1,\n",
    "):\n",
    "    \"\"\"Training loop for adaptive proposal q_phi\n",
    "\n",
    "    Training loop for jointly learning p_tilde_theta and q_phi.\n",
    "    Where we assume that we can sample and evaluate q_phi.\n",
    "    \"\"\"\n",
    "    p_optimizer = torch.optim.SGD(p_theta.parameters(), lr=lr)\n",
    "    q_optimizer = torch.optim.SGD(q_phi.parameters(), lr=lr)\n",
    "    batch_metrics = []\n",
    "    batch_metrics.append(evaluation_metric(p_theta))\n",
    "    batch_losses = []\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        # print(f\"Epoch {epoch}\")\n",
    "        old_params = torch.nn.utils.parameters_to_vector(q_phi.parameters())\n",
    "        for _, (y, idx) in enumerate(train_loader, 0):\n",
    "\n",
    "            #with torch.no_grad():\n",
    "            #    p_loss = p_criterion.crit(y, None)\n",
    "            #    batch_losses.append(p_loss.item())\n",
    "            # Calculate and assign gradients\n",
    "            p_optimizer.zero_grad()\n",
    "            p_criterion.calculate_crit_grad(y, idx)\n",
    "            p_optimizer.step()\n",
    "\n",
    "            q_optimizer.zero_grad()\n",
    "            q_criterion.calculate_crit_grad(y, idx)\n",
    "            q_optimizer.step()\n",
    "            with torch.no_grad():\n",
    "                batch_metrics.append(evaluation_metric(p_theta))\n",
    "            \n",
    "        if stopping_condition(\n",
    "            torch.nn.utils.parameters_to_vector(q_phi.parameters()), old_params\n",
    "        ):\n",
    "            print(\"Training converged\")\n",
    "            break\n",
    "    return torch.tensor(batch_losses), torch.tensor(batch_metrics)\n",
    "\n",
    "p_theta = DiagGaussianModel(init_mu.clone(), init_cov.clone())\n",
    "q_phi = AdaptiveDiagGaussianModel(mu_star.clone(), cov_star.clone())\n",
    "p_crit = NceRankCrit(p_theta, q_phi, J)\n",
    "q_crit = AdaptiveRankKernel(p_theta, q_phi, J)\n",
    "\n",
    "training_data = MultivariateNormalData(mu_star, cov_star, N)\n",
    "train_loader = torch.utils.data.DataLoader(training_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "adaptive_losses, adaptive_metrics = train_model_adaptive_proposal(p_theta,\n",
    "                                                        q_phi,\n",
    "                                                        p_crit,\n",
    "                                                        q_crit,\n",
    "                                                        metric,\n",
    "                                                        train_loader,\n",
    "                                                        None,\n",
    "                                                        neg_sample_size=J,\n",
    "                                                        num_epochs=num_epochs,\n",
    "                                                        stopping_condition=no_stopping,\n",
    "                                                        lr=learn_rate)\n",
    "# plot_distrs_adaptive(p_d, p_theta, q_phi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a41ef60",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "iters = torch.arange(start=0, end=p_d_metrics.size(0), step=10)\n",
    "iters = torch.arange(start=0, end=2050, step=10)\n",
    "ax.plot(iters, p_d_metrics[iters], label=\"$q=p_d$\")\n",
    "#ax.plot(iters, p_t_metrics[iters], label=\"$q=p_{\\\\theta}$\")\n",
    "#ax.plot(iters, adaptive_metrics[iters], label=\"$q=q_{\\\\varphi}$\")\n",
    "ax.legend();\n",
    "# ax.set_xlim([0, 250])\n",
    "ax.set_title(\"Choice of proposal distribution\")\n",
    "ax.set_xlabel(\"Iter. step $t$\")\n",
    "ax.set_ylabel(\"KL$(p_d || p_{\\\\theta})$\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b19122",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
