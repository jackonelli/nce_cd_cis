{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "006eb13f",
   "metadata": {},
   "source": [
    "# Choice of proposal distribution\n",
    "\n",
    "We investigate the effect of the proposal distribution when learning an unnormalised model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "626e8a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch\n",
    "from scipy.linalg import sqrtm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "from src.nce.cnce import CondNceCrit\n",
    "from src.nce.rank import NceRankCrit\n",
    "\n",
    "from src.noise_distr.normal import MultivariateNormal\n",
    "from src.models.gaussian_model import DiagGaussianModel\n",
    "\n",
    "from src.training.model_training import train_model, train_model_model_proposal\n",
    "from src.data.normal import MultivariateNormalData\n",
    "from src.training.training_utils import Mse, MvnKlDiv, no_change_stopping_condition, no_stopping\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "def mvn_curve(mu, cov, std=1, res=100):\n",
    "    with torch.no_grad():\n",
    "        angles = torch.linspace(0, 2*torch.pi, res)\n",
    "        curve_param = torch.column_stack((torch.cos(angles), torch.sin(angles)))\n",
    "        ellipsis = std * curve_param @ torch.Tensor(sqrtm(cov))\n",
    "        return mu + ellipsis\n",
    "    \n",
    "def plot_mvn(levels, ax, label):\n",
    "    ax.plot(levels[:, 0], levels[:, 1], label=label)\n",
    "    \n",
    "def plot_distrs(p_d, p_t_d, p_t_t):    \n",
    "    fig, ax = plt.subplots()\n",
    "    ax.set_xlim([-3, 10])\n",
    "    ax.set_ylim([-3, 10])\n",
    "    plot_mvn(mvn_curve(p_d.mu, p_d.cov), ax, \"$p_{d}}$\")\n",
    "    plot_mvn(mvn_curve(p_t_d.mu, p_t_d.cov()), ax, \"$p_{\\\\theta}, p_n = p_d$\")\n",
    "    plot_mvn(mvn_curve(p_t_t.mu, p_t_t.cov()), ax, \"$p_{\\\\theta}, p_n = p_{\\\\theta}$\")\n",
    "\n",
    "    ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f85269ee",
   "metadata": {},
   "source": [
    "# Idealistic case\n",
    "\n",
    "Assume that we can evaluate and sample from $p_d$ and $p_\\theta$,\n",
    "which is the better alternative as the proposal distribution $q$? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ccd2c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "D, N, J = 2, 100, 10 # Dimension, Num. data samples, Num neg. samples\n",
    "# Data model\n",
    "mu_star, cov_star = torch.ones(D,), torch.eye(D)\n",
    "p_d = MultivariateNormal(mu_star, cov_star)\n",
    "\n",
    "init_mu, init_cov =5.0*torch.ones(D,), 4*torch.eye(D)\n",
    "\n",
    "num_epochs = 200\n",
    "batch_size = 20\n",
    "learn_rate = 0.01*batch_size**0.5\n",
    "\n",
    "training_data = MultivariateNormalData(mu_star, cov_star, N)\n",
    "train_loader = torch.utils.data.DataLoader(training_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Metrics\n",
    "kl_div = MvnKlDiv(p_d.mu, p_d.cov).metric\n",
    "mse = Mse(p_d.mu).metric\n",
    "metric = kl_div\n",
    "\n",
    "# q = p_d\n",
    "p_t_data_noise = DiagGaussianModel(init_mu.clone(), init_cov.clone())\n",
    "criterion = NceRankCrit(p_t_data_noise, p_d, J)\n",
    "p_d_losses, p_d_metrics = train_model(criterion,\n",
    "                  metric,\n",
    "                  train_loader,\n",
    "                  None,\n",
    "                  neg_sample_size=J,\n",
    "                  num_epochs=num_epochs,\n",
    "                  stopping_condition=no_change_stopping_condition,\n",
    "                  lr=learn_rate)\n",
    "\n",
    "# q = p_theta\n",
    "p_t_model_noise = DiagGaussianModel(init_mu.clone(), init_cov.clone())\n",
    "p_t_losses, p_t_metrics = train_model_model_proposal(p_t_model_noise,\n",
    "                           NceRankCrit,\n",
    "                           metric,\n",
    "                           train_loader,\n",
    "                           None,\n",
    "                           J,\n",
    "                           num_epochs,\n",
    "                           lr=learn_rate)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(torch.arange(p_d_metrics.size(0)), p_d_metrics, label=\"$q=p_d$\")\n",
    "ax.plot(torch.arange(p_t_metrics.size(0)), p_t_metrics, label=\"$q=p_{\\\\theta}$\")\n",
    "ax.legend();\n",
    "ax.set_title(\"Choice of proposal distribution\")\n",
    "ax.set_xlabel(\"Iter. step $t$\")\n",
    "ax.set_ylabel(\"KL$(p_d || p_{\\\\theta})$\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae63a3f",
   "metadata": {},
   "source": [
    "# Adaptive proposal\n",
    "\n",
    "Assume that we have a learnable proposal $q_\\varphi$.\n",
    "We jointly learn this proposal by minimising\n",
    "$$\n",
    "KL(p_\\theta \\| q_\\varphi) \\propto - \\mathbb{E}_{x \\sim p_\\theta} \\log q_\\varphi(x) = \\mathcal{L}_\\varphi\n",
    "$$\n",
    "with,\n",
    "$$\n",
    "\\nabla_\\varphi \\mathcal{L}_\\varphi \\approx \\sum_{j=0}^J w(x_j) \\log \\nabla q_\\varphi(x_j)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e44b2b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_adaptive_proposal(\n",
    "    p_theta,\n",
    "    q_phi,\n",
    "    crit_constructor,\n",
    "    evaluation_metric,\n",
    "    train_loader,\n",
    "    save_dir,\n",
    "    neg_sample_size: int = 10,\n",
    "    num_epochs: int = 100,\n",
    "    stopping_condition=no_stopping,\n",
    "    lr: float = 0.1,\n",
    "):\n",
    "    \"\"\"Training loop for adaptive proposal q_phi\n",
    "\n",
    "    Training loop for jointly learning p_tilde_theta and q_phi.\n",
    "    Where we assume that we can sample and evaluate q_phi.\n",
    "    \"\"\"\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "    batch_metrics = []\n",
    "    batch_metrics.append(evaluation_metric(model))\n",
    "    batch_losses = []\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        # print(f\"Epoch {epoch}\")\n",
    "        old_params = torch.nn.utils.parameters_to_vector(model.parameters())\n",
    "        for _, (y, idx) in enumerate(train_loader, 0):\n",
    "            q = MultivariateNormal(\n",
    "                model.mu.detach().clone(), model.cov().clone().detach().clone()\n",
    "            )\n",
    "            criterion = crit_constructor(model, q, neg_sample_size)\n",
    "            optimizer.zero_grad()\n",
    "            with torch.no_grad():\n",
    "                loss = criterion.crit(y, None)\n",
    "                batch_losses.append(loss.item())\n",
    "                # print(loss)\n",
    "            # Calculate and assign gradients\n",
    "            criterion.calculate_crit_grad(y, idx)\n",
    "\n",
    "            # Take gradient step\n",
    "            optimizer.step()\n",
    "\n",
    "            # running_loss += loss.item()\n",
    "            batch_metrics.append(evaluation_metric(model))\n",
    "        if stopping_condition(\n",
    "            torch.nn.utils.parameters_to_vector(model.parameters()), old_params\n",
    "        ):\n",
    "            print(\"Training converged\")\n",
    "            break\n",
    "    return torch.tensor(batch_losses), torch.tensor(batch_metrics)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
