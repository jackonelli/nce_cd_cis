{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b21d34e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "from src.nce.cd_cnce import CdCnceCrit\n",
    "from src.nce.cd_mh import CdMHCrit\n",
    "\n",
    "from src.noise_distr.conditional_normal import ConditionalMultivariateNormal\n",
    "\n",
    "from src.models.rbm.rbm import Rbm\n",
    "from src.data.mnist import MnistDataset\n",
    "\n",
    "from src.training.model_training import train_model\n",
    "from src.training.training_utils import no_stopping\n",
    "\n",
    "from src.experiments.noise_distr_utils import get_nce_noise_distr_par, get_cnce_noise_distr_par\n",
    "from src.experiments.mnist_exp_utils import initialise_params\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4307bfe",
   "metadata": {},
   "source": [
    "## EXPERIMENTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fa4eb4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data specs\n",
    "num_dims = 28**2\n",
    "\n",
    "# Experiments specs\n",
    "num_neg_samples = 2\n",
    "\n",
    "# Training specs\n",
    "batch_size = 100\n",
    "num_epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "40698ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check accuracy of model\n",
    "def rbm_acc(rbm, data_loader, k=1):\n",
    "    acc = 0\n",
    "    for i, (y, idx) in enumerate(data_loader, 0):\n",
    "        y_pred = rbm.sample(y, k=k)\n",
    "        acc += torch.mean(y_pred == y, dim=-1)\n",
    "        \n",
    "    return acc / len(data_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "655c310d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurations to consider in experiments \n",
    "\n",
    "config_conditional_multi = {\n",
    "    \"criterion\": CdCnceCrit,\n",
    "    \"label\": \"cd_cnce\",\n",
    "    \"estimate_part_fn\": False,\n",
    "    \"conditional_noise_distr\": True,\n",
    "    \"mcmc_steps\": 1,\n",
    "}\n",
    "\n",
    "configs = [config_conditional_multi]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e82c8c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get data \n",
    "training_data = MnistDataset(train=True, root_dir=\"src/data/datasets/\")\n",
    "train_loader = torch.utils.data.DataLoader(training_data, batch_size=batch_size, shuffle=True)\n",
    "                             \n",
    "test_data = MnistDataset(train=False, root_dir=\"src/data/datasets/\")\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "090b9998",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "t() expects a tensor with <= 2 dimensions, but self is 3D",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[1;32mIn [23]\u001b[0m, in \u001b[0;36m<cell line: 11>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Conditional\u001b[39;00m\n\u001b[0;32m     10\u001b[0m p_m \u001b[38;5;241m=\u001b[39m Rbm(weights\u001b[38;5;241m=\u001b[39mweights\u001b[38;5;241m.\u001b[39mclone(), vis_bias\u001b[38;5;241m=\u001b[39mvis_bias\u001b[38;5;241m.\u001b[39mclone(), hidden_bias\u001b[38;5;241m=\u001b[39mhidden_bias\u001b[38;5;241m.\u001b[39mclone())\n\u001b[1;32m---> 11\u001b[0m cov_noise_cnce \u001b[38;5;241m=\u001b[39m \u001b[43mget_cnce_noise_distr_par\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtraining_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_full_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_neg_samples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp_m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\PycharmProjects\\deep_ext_obj\\nbs\\..\\src\\experiments\\noise_distr_utils.py:34\u001b[0m, in \u001b[0;36mget_cnce_noise_distr_par\u001b[1;34m(y, neg_sample_size, p_m)\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_cnce_noise_distr_par\u001b[39m(y, neg_sample_size, p_m):\n\u001b[1;32m---> 34\u001b[0m     epsilon_factor \u001b[38;5;241m=\u001b[39m \u001b[43mget_cnce_epsilon_factor\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mneg_sample_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp_m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     35\u001b[0m     epsilon_base \u001b[38;5;241m=\u001b[39m get_cnce_epsilon_base(y)\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m get_cnce_covariance_matrix(epsilon_factor, epsilon_base, y\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n",
      "File \u001b[1;32m~\\PycharmProjects\\deep_ext_obj\\nbs\\..\\src\\experiments\\noise_distr_utils.py:59\u001b[0m, in \u001b[0;36mget_cnce_epsilon_factor\u001b[1;34m(y, neg_sample_size, p_m, thrs_lower, thrs_upper, inc_rate, dec_rate, max_iter, eps_hard_cap)\u001b[0m\n\u001b[0;32m     56\u001b[0m epsilon_factor \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.5\u001b[39m  \u001b[38;5;66;03m# Start value\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;66;03m# Calculate initial loss\u001b[39;00m\n\u001b[1;32m---> 59\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_cnce_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepsilon_factor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mneg_sample_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp_m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;66;03m# Iterate until conditions are met\u001b[39;00m\n\u001b[0;32m     62\u001b[0m k \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32m~\\PycharmProjects\\deep_ext_obj\\nbs\\..\\src\\experiments\\noise_distr_utils.py:46\u001b[0m, in \u001b[0;36mevaluate_cnce_loss\u001b[1;34m(y, epsilon_factor, neg_sample_size, p_m)\u001b[0m\n\u001b[0;32m     43\u001b[0m p_n \u001b[38;5;241m=\u001b[39m ConditionalMultivariateNormal(cov\u001b[38;5;241m=\u001b[39mcov_noise)\n\u001b[0;32m     44\u001b[0m criterion \u001b[38;5;241m=\u001b[39m CondNceCrit(p_m, p_n, neg_sample_size)\n\u001b[1;32m---> 46\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcriterion\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcrit\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\PycharmProjects\\deep_ext_obj\\nbs\\..\\src\\nce\\cnce.py:17\u001b[0m, in \u001b[0;36mCondNceCrit.crit\u001b[1;34m(self, y, _idx)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcrit\u001b[39m(\u001b[38;5;28mself\u001b[39m, y: Tensor, _idx: Optional[Tensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m     15\u001b[0m     y_samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msample_noise(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_neg, y)\n\u001b[1;32m---> 17\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minner_crit\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_samples\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\PycharmProjects\\deep_ext_obj\\nbs\\..\\src\\nce\\cnce.py:21\u001b[0m, in \u001b[0;36mCondNceCrit.inner_crit\u001b[1;34m(self, y, y_samples)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner_crit\u001b[39m(\u001b[38;5;28mself\u001b[39m, y: Tensor, y_samples):\n\u001b[1;32m---> 21\u001b[0m     log_w_tilde \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_log_unnorm_w\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_samples\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m torch\u001b[38;5;241m.\u001b[39mexp(\u001b[38;5;241m-\u001b[39mlog_w_tilde))\u001b[38;5;241m.\u001b[39mmean()\n",
      "File \u001b[1;32m~\\PycharmProjects\\deep_ext_obj\\nbs\\..\\src\\nce\\cnce.py:35\u001b[0m, in \u001b[0;36mCondNceCrit._log_unnorm_w\u001b[1;34m(self, y, y_samples)\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_log_unnorm_w\u001b[39m(\u001b[38;5;28mself\u001b[39m, y, y_samples) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m     33\u001b[0m     \u001b[38;5;124;03m\"\"\"Log weights of y (NxD) and y_samples (NxJxD)\"\"\"\u001b[39;00m\n\u001b[1;32m---> 35\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlog_cond_unnorm_weights\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     36\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     37\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     38\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_unnorm_distr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog_prob\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     39\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_noise_distr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog_prob\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     40\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\PycharmProjects\\deep_ext_obj\\nbs\\..\\src\\part_fn_utils.py:39\u001b[0m, in \u001b[0;36mlog_cond_unnorm_weights\u001b[1;34m(y, yp, log_unnorm_distr, log_noise_distr)\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlog_cond_unnorm_weights\u001b[39m(\n\u001b[0;32m     36\u001b[0m     y: Tensor, yp: Tensor, log_unnorm_distr, log_noise_distr\n\u001b[0;32m     37\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m     38\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m---> 39\u001b[0m         \u001b[43mlog_unnorm_distr\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     40\u001b[0m         \u001b[38;5;241m+\u001b[39m log_noise_distr(yp, y)\n\u001b[0;32m     41\u001b[0m         \u001b[38;5;241m-\u001b[39m log_unnorm_distr(yp)\n\u001b[0;32m     42\u001b[0m         \u001b[38;5;241m-\u001b[39m log_noise_distr(y, yp)\n\u001b[0;32m     43\u001b[0m     )\n",
      "File \u001b[1;32m~\\PycharmProjects\\deep_ext_obj\\nbs\\..\\src\\models\\rbm\\rbm.py:23\u001b[0m, in \u001b[0;36mRbm.log_prob\u001b[1;34m(self, y)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlog_prob\u001b[39m(\u001b[38;5;28mself\u001b[39m, y: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m     21\u001b[0m     _, h \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msample_hidden(y)\n\u001b[1;32m---> 23\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mlog(torch\u001b[38;5;241m.\u001b[39mexp(\u001b[38;5;241m-\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menergy\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh\u001b[49m\u001b[43m)\u001b[49m))\n",
      "File \u001b[1;32m~\\PycharmProjects\\deep_ext_obj\\nbs\\..\\src\\models\\rbm\\rbm.py:27\u001b[0m, in \u001b[0;36mRbm.energy\u001b[1;34m(self, v, h)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21menergy\u001b[39m(\u001b[38;5;28mself\u001b[39m, v: Tensor, h: Tensor):\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mallclose((v \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39mmatmul(h, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights\u001b[38;5;241m.\u001b[39mt()))\u001b[38;5;241m.\u001b[39msum(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m),\n\u001b[1;32m---> 27\u001b[0m                           torch\u001b[38;5;241m.\u001b[39mdiag(torch\u001b[38;5;241m.\u001b[39mmatmul(v, torch\u001b[38;5;241m.\u001b[39mmatmul(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights, \u001b[43mh\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m))))\n\u001b[0;32m     29\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m-\u001b[39m torch\u001b[38;5;241m.\u001b[39mmatmul(v, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvis_bias) \u001b[38;5;241m-\u001b[39m torch\u001b[38;5;241m.\u001b[39mmatmul(h, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_bias) \\\n\u001b[0;32m     30\u001b[0m            \u001b[38;5;241m-\u001b[39m (v \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39mmatmul(h, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights\u001b[38;5;241m.\u001b[39mt()))\u001b[38;5;241m.\u001b[39msum(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: t() expects a tensor with <= 2 dimensions, but self is 3D"
     ]
    }
   ],
   "source": [
    "# Initialise           \n",
    "weights, vis_bias, hidden_bias = initialise_params()\n",
    "\n",
    "# Get noise distr. params\n",
    "\n",
    "# Unconditional\n",
    "mu_noise_nce, cov_noise_nce = get_nce_noise_distr_par(training_data.get_full_data())\n",
    "\n",
    "# Conditional\n",
    "p_m = Rbm(weights=weights.clone(), vis_bias=vis_bias.clone(), hidden_bias=hidden_bias.clone())\n",
    "cov_noise_cnce = get_cnce_noise_distr_par(training_data.get_full_data(), num_neg_samples, p_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a08bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run experiments\n",
    "\n",
    "for config in configs:\n",
    "\n",
    "    # Make sure that these are \"reinitialised\"\n",
    "    p_m, p_n, criterion = None, None, None\n",
    "\n",
    "    p_m = Rbm(weights=weights.clone(), vis_bias=vis_bias.clone(), hidden_bias=hidden_bias.clone())\n",
    "    \n",
    "    if config[\"conditional_noise_distr\"]:\n",
    "        p_n = ConditionalMultivariateNormal(cov=cov_noise_cnce)\n",
    "    else:\n",
    "        p_n = MultivariateNormal(mu=mu_noise_nce, cov=cov_noise_nce)\n",
    "\n",
    "    if config[\"mcmc_steps\"] is not None:\n",
    "        criterion = config[\"criterion\"](p_m, p_n, num_neg_samples, config[\"mcmc_steps\"])\n",
    "    else:\n",
    "        criterion = config[\"criterion\"](p_m, p_n, num_neg_samples)\n",
    "\n",
    "    save_dir = None\n",
    "    _ = train_model(criterion, error_metric, train_loader, save_dir, neg_sample_size=num_neg_samples, num_epochs=num_epochs,\n",
    "                    decaying_lr=True, stopping_condition=no_stopping)\n",
    "    \n",
    "    # Check test accuracy of model\n",
    "    acc = rbm_acc(p_m test_loader):\n",
    "    print(\"Model accuracy: {}\".format(acc))\n",
    "    \n",
    "    # Save model\n",
    "    torch.save(p_m.state_dict(), \"res/params_\" + config[\"label\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489b6037",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise results\n",
    "\n",
    "num_samples = 4\n",
    "\n",
    "p_m = Rbm(weights=weights.clone(), vis_bias=vis_bias.clone(), hidden_bias=hidden_bias.clone())\n",
    "for config in configs:\n",
    "    p_m.load_state_dict(torch.load(\"res/params_\" + config[\"label\"]))\n",
    "    y_pred = p_m.sample(torch.randn((num_samples, num_dims)), k=1) # Eller hur fungerar detta?\n",
    "    \n",
    "    plt.imshow(np.transpose(torchvision.utils.make_grid(y_pred.reshape(-1, 1, 28, 28)).numpy(), (1, 2, 0)))\n",
    "    plt.title(config[\"label\"])\n",
    "    plt.show()\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
